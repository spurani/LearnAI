{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient using REINFORCE algorithm\n",
    "\n",
    "In this demo we create a multi-layer perceptron using tensorflow and train it to estimate our policy for the cart-pole environment in openai gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                num_outputs = 10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  29.0\n",
      "Mean Reward 29.0\n",
      "Max reward so far:  29.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  39.0\n",
      "Mean Reward 34.0\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  13.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  28.0\n",
      "Mean Reward 27.25\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  26.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  22.0\n",
      "Mean Reward 26.166666666666668\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  13.0\n",
      "Mean Reward 24.285714285714285\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  13.0\n",
      "Mean Reward 22.875\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  25.0\n",
      "Mean Reward 23.11111111111111\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  9.0\n",
      "Mean Reward 21.7\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  19.0\n",
      "Mean Reward 21.454545454545453\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  24.0\n",
      "Mean Reward 21.666666666666668\n",
      "Max reward so far:  39.0\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  40.0\n",
      "Mean Reward 23.076923076923077\n",
      "Max reward so far:  40.0\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  23.0\n",
      "Mean Reward 23.071428571428573\n",
      "Max reward so far:  40.0\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  23.0\n",
      "Mean Reward 23.066666666666666\n",
      "Max reward so far:  40.0\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  29.0\n",
      "Mean Reward 23.4375\n",
      "Max reward so far:  40.0\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  13.0\n",
      "Mean Reward 22.823529411764707\n",
      "Max reward so far:  40.0\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  68.0\n",
      "Mean Reward 25.333333333333332\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  22.0\n",
      "Mean Reward 25.157894736842106\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  41.0\n",
      "Mean Reward 25.95\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  41.0\n",
      "Mean Reward 26.666666666666668\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  58.0\n",
      "Mean Reward 28.09090909090909\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  28.0\n",
      "Mean Reward 28.08695652173913\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  10.0\n",
      "Mean Reward 27.333333333333332\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  60.0\n",
      "Mean Reward 28.64\n",
      "Max reward so far:  68.0\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  98.0\n",
      "Mean Reward 31.307692307692307\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  15.0\n",
      "Mean Reward 30.703703703703702\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  13.0\n",
      "Mean Reward 30.071428571428573\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  17.0\n",
      "Mean Reward 29.620689655172413\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  29.0\n",
      "Mean Reward 29.6\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  23.0\n",
      "Mean Reward 29.387096774193548\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  17.0\n",
      "Mean Reward 29.0\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  16.0\n",
      "Mean Reward 28.606060606060606\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  27.0\n",
      "Mean Reward 28.558823529411764\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  15.0\n",
      "Mean Reward 28.17142857142857\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  27.0\n",
      "Mean Reward 28.13888888888889\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  15.0\n",
      "Mean Reward 27.783783783783782\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  15.0\n",
      "Mean Reward 27.44736842105263\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  10.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  12.0\n",
      "Mean Reward 26.625\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  16.0\n",
      "Mean Reward 26.365853658536587\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  34.0\n",
      "Mean Reward 26.547619047619047\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  46.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  15.0\n",
      "Mean Reward 26.727272727272727\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  12.0\n",
      "Mean Reward 26.4\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  23.0\n",
      "Mean Reward 26.32608695652174\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  17.0\n",
      "Mean Reward 26.127659574468087\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  23.0\n",
      "Mean Reward 26.0625\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  18.0\n",
      "Mean Reward 25.897959183673468\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  12.0\n",
      "Mean Reward 25.62\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  20.0\n",
      "Mean Reward 25.50980392156863\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  16.0\n",
      "Mean Reward 25.326923076923077\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  24.0\n",
      "Mean Reward 25.30188679245283\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  33.0\n",
      "Mean Reward 25.444444444444443\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  31.0\n",
      "Mean Reward 25.545454545454547\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  14.0\n",
      "Mean Reward 25.339285714285715\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  16.0\n",
      "Mean Reward 25.17543859649123\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  11.0\n",
      "Mean Reward 24.93103448275862\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  21.0\n",
      "Mean Reward 24.864406779661017\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  24.0\n",
      "Mean Reward 24.85\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  31.0\n",
      "Mean Reward 24.950819672131146\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  65.0\n",
      "Mean Reward 25.596774193548388\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  19.0\n",
      "Mean Reward 25.49206349206349\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  32.0\n",
      "Mean Reward 25.59375\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  33.0\n",
      "Mean Reward 25.70769230769231\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  11.0\n",
      "Mean Reward 25.484848484848484\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  17.0\n",
      "Mean Reward 25.35820895522388\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  10.0\n",
      "Mean Reward 25.13235294117647\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  37.0\n",
      "Mean Reward 25.304347826086957\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  15.0\n",
      "Mean Reward 25.15714285714286\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  11.0\n",
      "Mean Reward 24.95774647887324\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  11.0\n",
      "Mean Reward 24.76388888888889\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  27.0\n",
      "Mean Reward 24.794520547945204\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  23.0\n",
      "Mean Reward 24.77027027027027\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  15.0\n",
      "Mean Reward 24.64\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  11.0\n",
      "Mean Reward 24.460526315789473\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  21.0\n",
      "Mean Reward 24.415584415584416\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  11.0\n",
      "Mean Reward 24.243589743589745\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  15.0\n",
      "Mean Reward 24.126582278481013\n",
      "Max reward so far:  98.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  21.0\n",
      "Mean Reward 24.0875\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  31.0\n",
      "Mean Reward 24.17283950617284\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  35.0\n",
      "Mean Reward 24.304878048780488\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  20.0\n",
      "Mean Reward 24.253012048192772\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  9.0\n",
      "Mean Reward 24.071428571428573\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  37.0\n",
      "Mean Reward 24.223529411764705\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  14.0\n",
      "Mean Reward 24.1046511627907\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  17.0\n",
      "Mean Reward 24.022988505747126\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  42.0\n",
      "Mean Reward 24.227272727272727\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  27.0\n",
      "Mean Reward 24.258426966292134\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  11.0\n",
      "Mean Reward 24.11111111111111\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  32.0\n",
      "Mean Reward 24.197802197802197\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  21.0\n",
      "Mean Reward 24.16304347826087\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  34.0\n",
      "Mean Reward 24.268817204301076\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  16.0\n",
      "Mean Reward 24.180851063829788\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  20.0\n",
      "Mean Reward 24.13684210526316\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  26.0\n",
      "Mean Reward 24.15625\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  19.0\n",
      "Mean Reward 24.103092783505154\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  34.0\n",
      "Mean Reward 24.20408163265306\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  16.0\n",
      "Mean Reward 24.12121212121212\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  37.0\n",
      "Mean Reward 24.25\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  16.0\n",
      "Mean Reward 24.168316831683168\n",
      "Max reward so far:  98.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  46.0\n",
      "Mean Reward 24.38235294117647\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  22.0\n",
      "Mean Reward 24.359223300970875\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  29.0\n",
      "Mean Reward 24.403846153846153\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  29.0\n",
      "Mean Reward 24.447619047619046\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  46.0\n",
      "Mean Reward 24.650943396226417\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  13.0\n",
      "Mean Reward 24.542056074766354\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  19.0\n",
      "Mean Reward 24.49074074074074\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  26.0\n",
      "Mean Reward 24.504587155963304\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  38.0\n",
      "Mean Reward 24.62727272727273\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  19.0\n",
      "Mean Reward 24.576576576576578\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  11.0\n",
      "Mean Reward 24.455357142857142\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  77.0\n",
      "Mean Reward 24.920353982300885\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  10.0\n",
      "Mean Reward 24.789473684210527\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  36.0\n",
      "Mean Reward 24.88695652173913\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  13.0\n",
      "Mean Reward 24.78448275862069\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  22.0\n",
      "Mean Reward 24.76068376068376\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  38.0\n",
      "Mean Reward 24.872881355932204\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  33.0\n",
      "Mean Reward 24.941176470588236\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  26.0\n",
      "Mean Reward 24.95\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  14.0\n",
      "Mean Reward 24.859504132231404\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  14.0\n",
      "Mean Reward 24.770491803278688\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  19.0\n",
      "Mean Reward 24.723577235772357\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  32.0\n",
      "Mean Reward 24.782258064516128\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  12.0\n",
      "Mean Reward 24.68\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  16.0\n",
      "Mean Reward 24.61111111111111\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  50.0\n",
      "Mean Reward 24.811023622047244\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  19.0\n",
      "Mean Reward 24.765625\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  18.0\n",
      "Mean Reward 24.713178294573645\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  14.0\n",
      "Mean Reward 24.630769230769232\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  22.0\n",
      "Mean Reward 24.610687022900763\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  12.0\n",
      "Mean Reward 24.515151515151516\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  16.0\n",
      "Mean Reward 24.451127819548873\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  18.0\n",
      "Mean Reward 24.402985074626866\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  30.0\n",
      "Mean Reward 24.444444444444443\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  9.0\n",
      "Mean Reward 24.330882352941178\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  21.0\n",
      "Mean Reward 24.306569343065693\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  27.0\n",
      "Mean Reward 24.32608695652174\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  39.0\n",
      "Mean Reward 24.431654676258994\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  80.0\n",
      "Mean Reward 24.82857142857143\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  18.0\n",
      "Mean Reward 24.78014184397163\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  22.0\n",
      "Mean Reward 24.760563380281692\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  19.0\n",
      "Mean Reward 24.72027972027972\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  8.0\n",
      "Mean Reward 24.604166666666668\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  10.0\n",
      "Mean Reward 24.50344827586207\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  31.0\n",
      "Mean Reward 24.54794520547945\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  23.0\n",
      "Mean Reward 24.537414965986393\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  16.0\n",
      "Mean Reward 24.47972972972973\n",
      "Max reward so far:  98.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  20.0\n",
      "Mean Reward 24.449664429530202\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  11.0\n",
      "Mean Reward 24.36\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  12.0\n",
      "Mean Reward 24.278145695364238\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  27.0\n",
      "Mean Reward 24.29605263157895\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  96.0\n",
      "Mean Reward 24.764705882352942\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  23.0\n",
      "Mean Reward 24.753246753246753\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  15.0\n",
      "Mean Reward 24.690322580645162\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  24.0\n",
      "Mean Reward 24.685897435897434\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  36.0\n",
      "Mean Reward 24.75796178343949\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  22.0\n",
      "Mean Reward 24.740506329113924\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  15.0\n",
      "Mean Reward 24.67924528301887\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  59.0\n",
      "Mean Reward 24.89375\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  13.0\n",
      "Mean Reward 24.819875776397517\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  35.0\n",
      "Mean Reward 24.882716049382715\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  11.0\n",
      "Mean Reward 24.79754601226994\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  34.0\n",
      "Mean Reward 24.853658536585368\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  23.0\n",
      "Mean Reward 24.842424242424244\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  30.0\n",
      "Mean Reward 24.873493975903614\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  21.0\n",
      "Mean Reward 24.850299401197606\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  13.0\n",
      "Mean Reward 24.779761904761905\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  46.0\n",
      "Mean Reward 24.90532544378698\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  49.0\n",
      "Mean Reward 25.04705882352941\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  26.0\n",
      "Mean Reward 25.05263157894737\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  74.0\n",
      "Mean Reward 25.337209302325583\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  53.0\n",
      "Mean Reward 25.497109826589597\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  87.0\n",
      "Mean Reward 25.850574712643677\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  36.0\n",
      "Mean Reward 25.908571428571427\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  14.0\n",
      "Mean Reward 25.84090909090909\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  15.0\n",
      "Mean Reward 25.779661016949152\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  70.0\n",
      "Mean Reward 26.028089887640448\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  50.0\n",
      "Mean Reward 26.162011173184357\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  53.0\n",
      "Mean Reward 26.31111111111111\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  87.0\n",
      "Mean Reward 26.646408839779006\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  71.0\n",
      "Mean Reward 26.89010989010989\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  78.0\n",
      "Mean Reward 27.169398907103826\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  43.0\n",
      "Mean Reward 27.255434782608695\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  17.0\n",
      "Mean Reward 27.2\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  26.0\n",
      "Mean Reward 27.193548387096776\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  22.0\n",
      "Mean Reward 27.165775401069517\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  16.0\n",
      "Mean Reward 27.106382978723403\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  15.0\n",
      "Mean Reward 27.04232804232804\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  16.0\n",
      "Mean Reward 26.98421052631579\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  33.0\n",
      "Mean Reward 27.015706806282722\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  65.0\n",
      "Mean Reward 27.213541666666668\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  22.0\n",
      "Mean Reward 27.186528497409327\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  32.0\n",
      "Mean Reward 27.211340206185568\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  20.0\n",
      "Mean Reward 27.174358974358974\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  41.0\n",
      "Mean Reward 27.244897959183675\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  30.0\n",
      "Mean Reward 27.258883248730964\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  36.0\n",
      "Mean Reward 27.303030303030305\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  27.0\n",
      "Mean Reward 27.301507537688444\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  23.0\n",
      "Mean Reward 27.28\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  35.0\n",
      "Mean Reward 27.318407960199004\n",
      "Max reward so far:  98.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  69.0\n",
      "Mean Reward 27.524752475247524\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  32.0\n",
      "Mean Reward 27.54679802955665\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  67.0\n",
      "Mean Reward 27.74019607843137\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  68.0\n",
      "Mean Reward 27.93658536585366\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  98.0\n",
      "Mean Reward 28.276699029126213\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  77.0\n",
      "Mean Reward 28.51207729468599\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  22.0\n",
      "Mean Reward 28.48076923076923\n",
      "Max reward so far:  98.0\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  127.0\n",
      "Mean Reward 28.952153110047846\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  42.0\n",
      "Mean Reward 29.014285714285716\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  37.0\n",
      "Mean Reward 29.0521327014218\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  42.0\n",
      "Mean Reward 29.11320754716981\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  20.0\n",
      "Mean Reward 29.070422535211268\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  80.0\n",
      "Mean Reward 29.30841121495327\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  25.0\n",
      "Mean Reward 29.288372093023256\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  49.0\n",
      "Mean Reward 29.37962962962963\n",
      "Max reward so far:  127.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  59.0\n",
      "Mean Reward 29.516129032258064\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  54.0\n",
      "Mean Reward 29.628440366972477\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  79.0\n",
      "Mean Reward 29.85388127853881\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  40.0\n",
      "Mean Reward 29.9\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  91.0\n",
      "Mean Reward 30.176470588235293\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  44.0\n",
      "Mean Reward 30.23873873873874\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  113.0\n",
      "Mean Reward 30.609865470852018\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  18.0\n",
      "Mean Reward 30.553571428571427\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  28.0\n",
      "Mean Reward 30.54222222222222\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  56.0\n",
      "Mean Reward 30.654867256637168\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  16.0\n",
      "Mean Reward 30.590308370044053\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  97.0\n",
      "Mean Reward 30.88157894736842\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  67.0\n",
      "Mean Reward 31.03930131004367\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  132.0\n",
      "Mean Reward 31.47826086956522\n",
      "Max reward so far:  132.0\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  82.0\n",
      "Mean Reward 31.696969696969695\n",
      "Max reward so far:  132.0\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  114.0\n",
      "Mean Reward 32.05172413793103\n",
      "Max reward so far:  132.0\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  113.0\n",
      "Mean Reward 32.39914163090129\n",
      "Max reward so far:  132.0\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  178.0\n",
      "Mean Reward 33.02136752136752\n",
      "Max reward so far:  178.0\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  457.0\n",
      "Mean Reward 34.82553191489362\n",
      "Max reward so far:  457.0\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  26.0\n",
      "Mean Reward 34.78813559322034\n",
      "Max reward so far:  457.0\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  512.0\n",
      "Mean Reward 36.80168776371308\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  166.0\n",
      "Mean Reward 37.34453781512605\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  301.0\n",
      "Mean Reward 38.44769874476987\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  144.0\n",
      "Mean Reward 38.8875\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  187.0\n",
      "Mean Reward 39.50207468879668\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  311.0\n",
      "Mean Reward 40.62396694214876\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  114.0\n",
      "Mean Reward 40.925925925925924\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  205.0\n",
      "Mean Reward 41.59836065573771\n",
      "Max reward so far:  512.0\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  539.0\n",
      "Mean Reward 43.628571428571426\n",
      "Max reward so far:  539.0\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  865.0\n",
      "Mean Reward 46.96747967479675\n",
      "Max reward so far:  865.0\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  577.0\n",
      "Mean Reward 49.11336032388664\n",
      "Max reward so far:  865.0\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  183.0\n",
      "Mean Reward 49.653225806451616\n",
      "Max reward so far:  865.0\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  451.0\n",
      "Mean Reward 51.265060240963855\n",
      "Max reward so far:  865.0\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  552.0\n",
      "Mean Reward 53.268\n",
      "Max reward so far:  865.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  835.0\n",
      "Mean Reward 56.38247011952191\n",
      "Max reward so far:  865.0\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  998.0\n",
      "Mean Reward 60.11904761904762\n",
      "Max reward so far:  998.0\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  399.0\n",
      "Mean Reward 61.458498023715414\n",
      "Max reward so far:  998.0\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  623.0\n",
      "Mean Reward 63.669291338582674\n",
      "Max reward so far:  998.0\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  280.0\n",
      "Mean Reward 64.51764705882353\n",
      "Max reward so far:  998.0\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  655.0\n",
      "Mean Reward 66.82421875\n",
      "Max reward so far:  998.0\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  1119.0\n",
      "Mean Reward 70.91828793774319\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  443.0\n",
      "Mean Reward 72.36046511627907\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  253.0\n",
      "Mean Reward 73.05791505791505\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  171.0\n",
      "Mean Reward 73.43461538461538\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  236.0\n",
      "Mean Reward 74.05747126436782\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  140.0\n",
      "Mean Reward 74.30916030534351\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  326.0\n",
      "Mean Reward 75.26615969581749\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  204.0\n",
      "Mean Reward 75.75378787878788\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  132.0\n",
      "Mean Reward 75.96603773584906\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  98.0\n",
      "Mean Reward 76.04887218045113\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  118.0\n",
      "Mean Reward 76.2059925093633\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  165.0\n",
      "Mean Reward 76.53731343283582\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  157.0\n",
      "Mean Reward 76.8364312267658\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  145.0\n",
      "Mean Reward 77.08888888888889\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  311.0\n",
      "Mean Reward 77.9520295202952\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  172.0\n",
      "Mean Reward 78.29779411764706\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  220.0\n",
      "Mean Reward 78.81684981684981\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  235.0\n",
      "Mean Reward 79.38686131386861\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  158.0\n",
      "Mean Reward 79.67272727272727\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  263.0\n",
      "Mean Reward 80.33695652173913\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  217.0\n",
      "Mean Reward 80.83032490974729\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  401.0\n",
      "Mean Reward 81.9820143884892\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  427.0\n",
      "Mean Reward 83.21863799283155\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  229.0\n",
      "Mean Reward 83.73928571428571\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  384.0\n",
      "Mean Reward 84.80782918149467\n",
      "Max reward so far:  1119.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  385.0\n",
      "Mean Reward 85.87234042553192\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  235.0\n",
      "Mean Reward 86.39929328621908\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  251.0\n",
      "Mean Reward 86.97887323943662\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  278.0\n",
      "Mean Reward 87.64912280701755\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  230.0\n",
      "Mean Reward 88.14685314685315\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  260.0\n",
      "Mean Reward 88.74564459930313\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  290.0\n",
      "Mean Reward 89.44444444444444\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  268.0\n",
      "Mean Reward 90.06228373702422\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  229.0\n",
      "Mean Reward 90.54137931034482\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  214.0\n",
      "Mean Reward 90.96563573883161\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  218.0\n",
      "Mean Reward 91.40068493150685\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  262.0\n",
      "Mean Reward 91.98293515358361\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  281.0\n",
      "Mean Reward 92.62585034013605\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  181.0\n",
      "Mean Reward 92.92542372881356\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  195.0\n",
      "Mean Reward 93.27027027027027\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  279.0\n",
      "Mean Reward 93.89562289562289\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  215.0\n",
      "Mean Reward 94.30201342281879\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  187.0\n",
      "Mean Reward 94.61204013377926\n",
      "Max reward so far:  1119.0\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  229.0\n",
      "Mean Reward 95.06\n",
      "Max reward so far:  1119.0\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        #env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "\n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SP.000\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "Score 22.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score 23.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score 82.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score 20.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score 24.0\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score 14.0\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score 34.0\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score 90.0\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score 31.0\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score 13.0\n",
      "Score over time: 35.3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            #print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
