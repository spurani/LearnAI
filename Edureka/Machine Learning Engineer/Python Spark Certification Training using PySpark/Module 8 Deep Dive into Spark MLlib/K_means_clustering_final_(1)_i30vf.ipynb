{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[0, 33.3, -17.5],\n",
    "                              [1, 40.4, -20.5],\n",
    "                              [2, 28., -23.9],\n",
    "                              [3, 29.5, -19.0],\n",
    "                              [4, 32.8, -18.84]\n",
    "                             ],[\"other\",\"lat\", \"long\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|other| lat|  long|\n",
      "+-----+----+------+\n",
      "|    0|33.3| -17.5|\n",
      "|    1|40.4| -20.5|\n",
      "|    2|28.0| -23.9|\n",
      "|    3|29.5| -19.0|\n",
      "|    4|32.8|-18.84|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler #TAKE A SET OF COL AND DEFINE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-------------+\n",
      "|other| lat|  long|     features|\n",
      "+-----+----+------+-------------+\n",
      "|    0|33.3| -17.5| [33.3,-17.5]|\n",
      "|    1|40.4| -20.5| [40.4,-20.5]|\n",
      "|    2|28.0| -23.9| [28.0,-23.9]|\n",
      "|    3|29.5| -19.0| [29.5,-19.0]|\n",
      "|    4|32.8|-18.84|[32.8,-18.84]|\n",
      "+-----+----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-------------+----------+\n",
      "|other| lat|  long|     features|prediction|\n",
      "+-----+----+------+-------------+----------+\n",
      "|    0|33.3| -17.5| [33.3,-17.5]|         0|\n",
      "|    1|40.4| -20.5| [40.4,-20.5]|         1|\n",
      "|    2|28.0| -23.9| [28.0,-23.9]|         0|\n",
      "|    3|29.5| -19.0| [29.5,-19.0]|         0|\n",
      "|    4|32.8|-18.84|[32.8,-18.84]|         0|\n",
      "+-----+----+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(new_df)\n",
    "transformed.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lat=33.3, long=-17.5),\n",
       " Row(lat=40.4, long=-20.5),\n",
       " Row(lat=28.0, long=-23.9),\n",
       " Row(lat=29.5, long=-19.0),\n",
       " Row(lat=32.8, long=-18.84)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('lat', 'long').rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33.3, -17.5), (40.4, -20.5), (28.0, -23.9), (29.5, -19.0), (32.8, -18.84)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('lat', 'long').rdd.map(lambda x: (x[0], x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "rdd = df.select('lat', 'long').rdd.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "clusters = KMeans.train(rdd, 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 36.85, -19.  ]), array([ 30.1 , -20.58])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
