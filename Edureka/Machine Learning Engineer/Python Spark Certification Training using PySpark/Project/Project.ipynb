{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, lit, unix_timestamp, from_unixtime, date_format, to_timestamp, hour, minute, second, dayofmonth, month, year\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "                    .appName('Bicycle') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|casual|registered|count|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|2011-01-01 00:00:00|     1|      0|         0|      1| 9.84|14.395|      81|        0|     3|        13|   16|\n",
      "|2011-01-01 01:00:00|     1|      0|         0|      1| 9.02|13.635|      80|        0|     8|        32|   40|\n",
      "|2011-01-01 02:00:00|     1|      0|         0|      1| 9.02|13.635|      80|        0|     5|        27|   32|\n",
      "|2011-01-01 03:00:00|     1|      0|         0|      1| 9.84|14.395|      75|        0|     3|        10|   13|\n",
      "|2011-01-01 04:00:00|     1|      0|         0|      1| 9.84|14.395|      75|        0|     0|         1|    1|\n",
      "|2011-01-01 05:00:00|     1|      0|         0|      2| 9.84| 12.88|      75|   6.0032|     0|         1|    1|\n",
      "|2011-01-01 06:00:00|     1|      0|         0|      1| 9.02|13.635|      80|        0|     2|         0|    2|\n",
      "|2011-01-01 07:00:00|     1|      0|         0|      1|  8.2| 12.88|      86|        0|     1|         2|    3|\n",
      "|2011-01-01 08:00:00|     1|      0|         0|      1| 9.84|14.395|      75|        0|     1|         7|    8|\n",
      "|2011-01-01 09:00:00|     1|      0|         0|      1|13.12|17.425|      76|        0|     8|         6|   14|\n",
      "|2011-01-01 10:00:00|     1|      0|         0|      1|15.58|19.695|      76|  16.9979|    12|        24|   36|\n",
      "|2011-01-01 11:00:00|     1|      0|         0|      1|14.76|16.665|      81|  19.0012|    26|        30|   56|\n",
      "|2011-01-01 12:00:00|     1|      0|         0|      1|17.22| 21.21|      77|  19.0012|    29|        55|   84|\n",
      "|2011-01-01 13:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.9995|    47|        47|   94|\n",
      "|2011-01-01 14:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.0012|    35|        71|  106|\n",
      "|2011-01-01 15:00:00|     1|      0|         0|      2|18.04| 21.97|      77|  19.9995|    40|        70|  110|\n",
      "|2011-01-01 16:00:00|     1|      0|         0|      2|17.22| 21.21|      82|  19.9995|    41|        52|   93|\n",
      "|2011-01-01 17:00:00|     1|      0|         0|      2|18.04| 21.97|      82|  19.0012|    15|        52|   67|\n",
      "|2011-01-01 18:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     9|        26|   35|\n",
      "|2011-01-01 19:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     6|        31|   37|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Data:\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+\n",
      "|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+\n",
      "|2011-01-20 00:00:00|     1|      0|         1|      1|10.66|11.365|      56|  26.0027|\n",
      "|2011-01-20 01:00:00|     1|      0|         1|      1|10.66|13.635|      56|        0|\n",
      "|2011-01-20 02:00:00|     1|      0|         1|      1|10.66|13.635|      56|        0|\n",
      "|2011-01-20 03:00:00|     1|      0|         1|      1|10.66| 12.88|      56|  11.0014|\n",
      "|2011-01-20 04:00:00|     1|      0|         1|      1|10.66| 12.88|      56|  11.0014|\n",
      "|2011-01-20 05:00:00|     1|      0|         1|      1| 9.84|11.365|      60|  15.0013|\n",
      "|2011-01-20 06:00:00|     1|      0|         1|      1| 9.02|10.605|      60|  15.0013|\n",
      "|2011-01-20 07:00:00|     1|      0|         1|      1| 9.02|10.605|      55|  15.0013|\n",
      "|2011-01-20 08:00:00|     1|      0|         1|      1| 9.02|10.605|      55|  19.0012|\n",
      "|2011-01-20 09:00:00|     1|      0|         1|      2| 9.84|11.365|      52|  15.0013|\n",
      "|2011-01-20 10:00:00|     1|      0|         1|      1|10.66|11.365|      48|  19.9995|\n",
      "|2011-01-20 11:00:00|     1|      0|         1|      2|11.48|13.635|      45|  11.0014|\n",
      "|2011-01-20 12:00:00|     1|      0|         1|      2| 12.3|16.665|      42|        0|\n",
      "|2011-01-20 13:00:00|     1|      0|         1|      2|11.48|14.395|      45|   7.0015|\n",
      "|2011-01-20 14:00:00|     1|      0|         1|      2| 12.3| 15.15|      45|   8.9981|\n",
      "|2011-01-20 15:00:00|     1|      0|         1|      2|13.12| 15.91|      45|   12.998|\n",
      "|2011-01-20 16:00:00|     1|      0|         1|      2| 12.3| 15.15|      49|   8.9981|\n",
      "|2011-01-20 17:00:00|     1|      0|         1|      2| 12.3| 15.91|      49|   7.0015|\n",
      "|2011-01-20 18:00:00|     1|      0|         1|      2|10.66| 12.88|      56|   12.998|\n",
      "|2011-01-20 19:00:00|     1|      0|         1|      1|10.66|11.365|      56|  22.0028|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##loading train dataset\n",
    "df_train = spark.read.option(\"header\",True)\\\n",
    "                  .format(\"csv\").load(\"train.csv\")\n",
    "#columns_to_drop = ['datetime','casual','registered','count']\n",
    "#df_train = df_train.drop(*columns_to_drop)\n",
    "print(\"Train Data:\")\n",
    "df_train.show()\n",
    "#df_train.createTempView(\"train\")\n",
    "###loading test dataset\n",
    "df_test = spark.read.option(\"header\",True)\\\n",
    "                  .format(\"csv\").load(\"test.csv\")\n",
    "#columns_to_drop_test = ['datetime']\n",
    "#df_test = df_test.drop(*columns_to_drop_test)\n",
    "#df_test.createTempView(\"test\")\n",
    "print(\"Test Data:\")\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datetime', 'string'),\n",
       " ('season', 'string'),\n",
       " ('holiday', 'string'),\n",
       " ('workingday', 'string'),\n",
       " ('weather', 'string'),\n",
       " ('temp', 'string'),\n",
       " ('atemp', 'string'),\n",
       " ('humidity', 'string'),\n",
       " ('windspeed', 'string'),\n",
       " ('casual', 'string'),\n",
       " ('registered', 'string'),\n",
       " ('count', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datetime', 'string'),\n",
       " ('season', 'string'),\n",
       " ('holiday', 'string'),\n",
       " ('workingday', 'string'),\n",
       " ('weather', 'string'),\n",
       " ('temp', 'string'),\n",
       " ('atemp', 'string'),\n",
       " ('humidity', 'string'),\n",
       " ('windspeed', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|           datetime|            season|            holiday|        workingday|           weather|              temp|            atemp|          humidity|         windspeed|           casual|        registered|             count|\n",
      "+-------+-------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              10886|             10886|              10886|             10886|             10886|             10886|            10886|             10886|             10886|            10886|             10886|             10886|\n",
      "|   mean|               null|2.5066139996325556|0.02856880396839978|0.6808745177291935| 1.418427337865148|20.230859819952173|23.65508405291192| 61.88645967297446|12.799395406945093|36.02195480433584| 155.5521771082124|191.57413191254824|\n",
      "| stddev|               null|1.1161743093443237|0.16659885062470944|0.4661591687997361|0.6338385858190968| 7.791589843987573| 8.47460062648494|19.245033277394704|  8.16453732683871|49.96047657264955|151.03903308192452|181.14445383028493|\n",
      "|    min|2011-01-01 00:00:00|                 1|                  0|                 0|                 1|              0.82|             0.76|                 0|                 0|                0|                 0|                 1|\n",
      "|    max|2012-12-19 23:00:00|                 4|                  1|                 1|                 4|              9.84|             9.85|                97|            8.9981|               99|                99|                99|\n",
      "+-------+-------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|           datetime|            season|             holiday|         workingday|           weather|              temp|             atemp|         humidity|        windspeed|\n",
      "+-------+-------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|               6493|              6493|                6493|               6493|              6493|              6493|              6493|             6493|             6493|\n",
      "|   mean|               null|  2.49330047743724|0.029108270445094717| 0.6858154936085015|1.4367780686893579|20.620606807330972|24.012864623440585| 64.1252117665178|12.63115720006173|\n",
      "| stddev|               null|1.0912579418644106| 0.16812296760854603|0.46422601479880476|0.6483898010717418| 8.059583026412682| 8.782741298669094|19.29339098607345|8.250151174075594|\n",
      "|    min|2011-01-20 00:00:00|                 1|                   0|                  0|                 1|              0.82|                 0|              100|                0|\n",
      "|    max|2012-12-31 23:00:00|                 4|                   1|                  1|                 4|              9.84|              9.85|               96|           8.9981|\n",
      "+-------+-------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, datetime: string, season: string, holiday: string, workingday: string, weather: string, temp: string, atemp: string, humidity: string, windspeed: string, casual: string, registered: string, count: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, datetime: string, season: string, holiday: string, workingday: string, weather: string, temp: string, atemp: string, humidity: string, windspeed: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+----------+-------+----+-----+--------+---------+------+----------+-----+\n",
      "|datetime|season|holiday|workingday|weather|temp|atemp|humidity|windspeed|casual|registered|count|\n",
      "+--------+------+-------+----------+-------+----+-----+--------+---------+------+----------+-----+\n",
      "|       0|     0|      0|         0|      0|   0|    0|       0|        0|     0|         0|    0|\n",
      "+--------+------+-------+----------+-------+----+-----+--------+---------+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Checking for Nan/Null values in train data\n",
    "df_train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_train.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+----------+-------+----+-----+--------+---------+\n",
      "|datetime|season|holiday|workingday|weather|temp|atemp|humidity|windspeed|\n",
      "+--------+------+-------+----------+-------+----+-----+--------+---------+\n",
      "|       0|     0|      0|         0|      0|   0|    0|       0|        0|\n",
      "+--------+------+-------+----------+-------+----+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Checking for Nan/Null values in test data\n",
    "df_test.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting Numerical variable of season to Categorical variable using StringIndexer and IndexToString\n",
    "##TRAINING DATASET\n",
    "df_train = (df_train.withColumn(\"season_1\", when(col(\"season\")==1,1).otherwise(0))\n",
    "                    .withColumn(\"season_2\", when(col(\"season\")==2,2).otherwise(0))\n",
    "                   .withColumn(\"season_3\", when(col(\"season\")==3,3).otherwise(0))\n",
    "                   .withColumn(\"season_4\", when(col(\"season\")==4,4).otherwise(0)))\n",
    "df_train = df_train.drop('season')\n",
    "\n",
    "##TESTING DATASET\n",
    "df_test = (df_test.withColumn(\"season_1\", when(col(\"season\")==1,1).otherwise(0))\n",
    "                    .withColumn(\"season_2\", when(col(\"season\")==2,2).otherwise(0))\n",
    "                   .withColumn(\"season_3\", when(col(\"season\")==3,3).otherwise(0))\n",
    "                   .withColumn(\"season_4\", when(col(\"season\")==4,4).otherwise(0)))\n",
    "df_test = df_test.drop('season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting Numerical variable of weather to Categorical variable using StringIndexer and IndexToString\n",
    "##TRAINING DATASET\n",
    "df_train = (df_train.withColumn(\"weather_1\", when(col(\"weather\")==1,1).otherwise(0))\n",
    "                   .withColumn(\"weather_2\", when(col(\"weather\")==2,2).otherwise(0))\n",
    "                   .withColumn(\"weather_3\", when(col(\"weather\")==3,3).otherwise(0))\n",
    "                   .withColumn(\"weather_4\", when(col(\"weather\")==4,4).otherwise(0)))\n",
    "df_train = df_train.drop('weather')\n",
    "\n",
    "##TESTING DATASET\n",
    "df_test = (df_test.withColumn(\"weather_1\", when(col(\"weather\")==1,1).otherwise(0))\n",
    "                   .withColumn(\"weather_2\", when(col(\"weather\")==2,2).otherwise(0))\n",
    "                   .withColumn(\"weather_3\", when(col(\"weather\")==3,3).otherwise(0))\n",
    "                   .withColumn(\"weather_4\", when(col(\"weather\")==4,4).otherwise(0)))\n",
    "df_test = df_test.drop('weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+-----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|           datetime|holiday|workingday| temp| atemp|humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|\n",
      "+-------------------+-------+----------+-----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|2011-01-01 00:00:00|      0|         0| 9.84|14.395|      81|        0|     3|        13|   16|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 01:00:00|      0|         0| 9.02|13.635|      80|        0|     8|        32|   40|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 02:00:00|      0|         0| 9.02|13.635|      80|        0|     5|        27|   32|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 03:00:00|      0|         0| 9.84|14.395|      75|        0|     3|        10|   13|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 04:00:00|      0|         0| 9.84|14.395|      75|        0|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 05:00:00|      0|         0| 9.84| 12.88|      75|   6.0032|     0|         1|    1|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-01 06:00:00|      0|         0| 9.02|13.635|      80|        0|     2|         0|    2|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 07:00:00|      0|         0|  8.2| 12.88|      86|        0|     1|         2|    3|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 08:00:00|      0|         0| 9.84|14.395|      75|        0|     1|         7|    8|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 09:00:00|      0|         0|13.12|17.425|      76|        0|     8|         6|   14|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 10:00:00|      0|         0|15.58|19.695|      76|  16.9979|    12|        24|   36|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 11:00:00|      0|         0|14.76|16.665|      81|  19.0012|    26|        30|   56|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 12:00:00|      0|         0|17.22| 21.21|      77|  19.0012|    29|        55|   84|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-01 13:00:00|      0|         0|18.86|22.725|      72|  19.9995|    47|        47|   94|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-01 14:00:00|      0|         0|18.86|22.725|      72|  19.0012|    35|        71|  106|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-01 15:00:00|      0|         0|18.04| 21.97|      77|  19.9995|    40|        70|  110|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-01 16:00:00|      0|         0|17.22| 21.21|      82|  19.9995|    41|        52|   93|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-01 17:00:00|      0|         0|18.04| 21.97|      82|  19.0012|    15|        52|   67|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-01 18:00:00|      0|         0|17.22| 21.21|      88|  16.9979|     9|        26|   35|       1|       0|       0|       0|        0|        0|        3|        0|\n",
      "|2011-01-01 19:00:00|      0|         0|17.22| 21.21|      88|  16.9979|     6|        31|   37|       1|       0|       0|       0|        0|        0|        3|        0|\n",
      "+-------------------+-------+----------+-----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+-----+------+--------+---------+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|           datetime|holiday|workingday| temp| atemp|humidity|windspeed|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|\n",
      "+-------------------+-------+----------+-----+------+--------+---------+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|2011-01-20 00:00:00|      0|         1|10.66|11.365|      56|  26.0027|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 01:00:00|      0|         1|10.66|13.635|      56|        0|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 02:00:00|      0|         1|10.66|13.635|      56|        0|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 03:00:00|      0|         1|10.66| 12.88|      56|  11.0014|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 04:00:00|      0|         1|10.66| 12.88|      56|  11.0014|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 05:00:00|      0|         1| 9.84|11.365|      60|  15.0013|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 06:00:00|      0|         1| 9.02|10.605|      60|  15.0013|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 07:00:00|      0|         1| 9.02|10.605|      55|  15.0013|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 08:00:00|      0|         1| 9.02|10.605|      55|  19.0012|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 09:00:00|      0|         1| 9.84|11.365|      52|  15.0013|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 10:00:00|      0|         1|10.66|11.365|      48|  19.9995|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "|2011-01-20 11:00:00|      0|         1|11.48|13.635|      45|  11.0014|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 12:00:00|      0|         1| 12.3|16.665|      42|        0|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 13:00:00|      0|         1|11.48|14.395|      45|   7.0015|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 14:00:00|      0|         1| 12.3| 15.15|      45|   8.9981|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 15:00:00|      0|         1|13.12| 15.91|      45|   12.998|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 16:00:00|      0|         1| 12.3| 15.15|      49|   8.9981|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 17:00:00|      0|         1| 12.3| 15.91|      49|   7.0015|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 18:00:00|      0|         1|10.66| 12.88|      56|   12.998|       1|       0|       0|       0|        0|        2|        0|        0|\n",
      "|2011-01-20 19:00:00|      0|         1|10.66|11.365|      56|  22.0028|       1|       0|       0|       0|        1|        0|        0|        0|\n",
      "+-------------------+-------+----------+-----+------+--------+---------+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convernting TRAINING DATASET datatypes from String to int\n",
    "df_train_new = df_train.withColumn('datetime',to_timestamp(\"datetime\"))\\\n",
    "                       .withColumn('holiday',df_train[\"holiday\"].cast(IntegerType()))\\\n",
    "                       .withColumn('workingday',df_train[\"workingday\"].cast(IntegerType()))\\\n",
    "                       .withColumn('temp',df_train[\"temp\"].cast(IntegerType()))\\\n",
    "                       .withColumn('atemp',df_train[\"atemp\"].cast(IntegerType()))\\\n",
    "                       .withColumn('humidity',df_train[\"humidity\"].cast(IntegerType()))\\\n",
    "                       .withColumn('windspeed',df_train[\"windspeed\"].cast(IntegerType()))\\\n",
    "                       .withColumn('casual',df_train[\"casual\"].cast(IntegerType()))\\\n",
    "                       .withColumn('registered',df_train[\"registered\"].cast(IntegerType()))\\\n",
    "                       .withColumn('count',df_train[\"count\"].cast(IntegerType()))\n",
    "\n",
    "df_test_new = df_test.withColumn('datetime',to_timestamp(\"datetime\"))\\\n",
    "                       .withColumn('holiday',df_test[\"holiday\"].cast(IntegerType()))\\\n",
    "                       .withColumn('workingday',df_test[\"workingday\"].cast(IntegerType()))\\\n",
    "                       .withColumn('temp',df_test[\"temp\"].cast(IntegerType()))\\\n",
    "                       .withColumn('atemp',df_test[\"atemp\"].cast(IntegerType()))\\\n",
    "                       .withColumn('humidity',df_test[\"humidity\"].cast(IntegerType()))\\\n",
    "                       .withColumn('windspeed',df_test[\"windspeed\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train_new)\n",
    "type(df_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('datetime', 'timestamp'), ('holiday', 'int'), ('workingday', 'int'), ('temp', 'int'), ('atemp', 'int'), ('humidity', 'int'), ('windspeed', 'int'), ('casual', 'int'), ('registered', 'int'), ('count', 'int'), ('season_1', 'int'), ('season_2', 'int'), ('season_3', 'int'), ('season_4', 'int'), ('weather_1', 'int'), ('weather_2', 'int'), ('weather_3', 'int'), ('weather_4', 'int')]\n",
      "[('datetime', 'timestamp'), ('holiday', 'int'), ('workingday', 'int'), ('temp', 'int'), ('atemp', 'int'), ('humidity', 'int'), ('windspeed', 'int'), ('season_1', 'int'), ('season_2', 'int'), ('season_3', 'int'), ('season_4', 'int'), ('weather_1', 'int'), ('weather_2', 'int'), ('weather_3', 'int'), ('weather_4', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(df_train_new.dtypes)\n",
    "print(df_test_new.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+\n",
      "|           datetime|holiday|workingday|temp|atemp|humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|minute|second|day|year|month|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+\n",
      "|2011-01-01 00:00:00|      0|         0|   9|   14|      81|        0|     3|        13|   16|       1|       0|       0|       0|        1|        0|        0|        0|   0|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 01:00:00|      0|         0|   9|   13|      80|        0|     8|        32|   40|       1|       0|       0|       0|        1|        0|        0|        0|   1|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 02:00:00|      0|         0|   9|   13|      80|        0|     5|        27|   32|       1|       0|       0|       0|        1|        0|        0|        0|   2|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 03:00:00|      0|         0|   9|   14|      75|        0|     3|        10|   13|       1|       0|       0|       0|        1|        0|        0|        0|   3|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 04:00:00|      0|         0|   9|   14|      75|        0|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|   4|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 05:00:00|      0|         0|   9|   12|      75|        6|     0|         1|    1|       1|       0|       0|       0|        0|        2|        0|        0|   5|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 06:00:00|      0|         0|   9|   13|      80|        0|     2|         0|    2|       1|       0|       0|       0|        1|        0|        0|        0|   6|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 07:00:00|      0|         0|   8|   12|      86|        0|     1|         2|    3|       1|       0|       0|       0|        1|        0|        0|        0|   7|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 08:00:00|      0|         0|   9|   14|      75|        0|     1|         7|    8|       1|       0|       0|       0|        1|        0|        0|        0|   8|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 09:00:00|      0|         0|  13|   17|      76|        0|     8|         6|   14|       1|       0|       0|       0|        1|        0|        0|        0|   9|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 10:00:00|      0|         0|  15|   19|      76|       16|    12|        24|   36|       1|       0|       0|       0|        1|        0|        0|        0|  10|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 11:00:00|      0|         0|  14|   16|      81|       19|    26|        30|   56|       1|       0|       0|       0|        1|        0|        0|        0|  11|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 12:00:00|      0|         0|  17|   21|      77|       19|    29|        55|   84|       1|       0|       0|       0|        1|        0|        0|        0|  12|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 13:00:00|      0|         0|  18|   22|      72|       19|    47|        47|   94|       1|       0|       0|       0|        0|        2|        0|        0|  13|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 14:00:00|      0|         0|  18|   22|      72|       19|    35|        71|  106|       1|       0|       0|       0|        0|        2|        0|        0|  14|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 15:00:00|      0|         0|  18|   21|      77|       19|    40|        70|  110|       1|       0|       0|       0|        0|        2|        0|        0|  15|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 16:00:00|      0|         0|  17|   21|      82|       19|    41|        52|   93|       1|       0|       0|       0|        0|        2|        0|        0|  16|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 17:00:00|      0|         0|  18|   21|      82|       19|    15|        52|   67|       1|       0|       0|       0|        0|        2|        0|        0|  17|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 18:00:00|      0|         0|  17|   21|      88|       16|     9|        26|   35|       1|       0|       0|       0|        0|        0|        3|        0|  18|     0|     0|  1|2011|    1|\n",
      "|2011-01-01 19:00:00|      0|         0|  17|   21|      88|       16|     6|        31|   37|       1|       0|       0|       0|        0|        0|        3|        0|  19|     0|     0|  1|2011|    1|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-------+----------+----+-----+--------+---------+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+\n",
      "|           datetime|holiday|workingday|temp|atemp|humidity|windspeed|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|minute|second|day|year|month|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+\n",
      "|2011-01-20 00:00:00|      0|         1|  10|   11|      56|       26|       1|       0|       0|       0|        1|        0|        0|        0|   0|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 01:00:00|      0|         1|  10|   13|      56|        0|       1|       0|       0|       0|        1|        0|        0|        0|   1|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 02:00:00|      0|         1|  10|   13|      56|        0|       1|       0|       0|       0|        1|        0|        0|        0|   2|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 03:00:00|      0|         1|  10|   12|      56|       11|       1|       0|       0|       0|        1|        0|        0|        0|   3|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 04:00:00|      0|         1|  10|   12|      56|       11|       1|       0|       0|       0|        1|        0|        0|        0|   4|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 05:00:00|      0|         1|   9|   11|      60|       15|       1|       0|       0|       0|        1|        0|        0|        0|   5|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 06:00:00|      0|         1|   9|   10|      60|       15|       1|       0|       0|       0|        1|        0|        0|        0|   6|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 07:00:00|      0|         1|   9|   10|      55|       15|       1|       0|       0|       0|        1|        0|        0|        0|   7|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 08:00:00|      0|         1|   9|   10|      55|       19|       1|       0|       0|       0|        1|        0|        0|        0|   8|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 09:00:00|      0|         1|   9|   11|      52|       15|       1|       0|       0|       0|        0|        2|        0|        0|   9|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 10:00:00|      0|         1|  10|   11|      48|       19|       1|       0|       0|       0|        1|        0|        0|        0|  10|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 11:00:00|      0|         1|  11|   13|      45|       11|       1|       0|       0|       0|        0|        2|        0|        0|  11|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 12:00:00|      0|         1|  12|   16|      42|        0|       1|       0|       0|       0|        0|        2|        0|        0|  12|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 13:00:00|      0|         1|  11|   14|      45|        7|       1|       0|       0|       0|        0|        2|        0|        0|  13|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 14:00:00|      0|         1|  12|   15|      45|        8|       1|       0|       0|       0|        0|        2|        0|        0|  14|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 15:00:00|      0|         1|  13|   15|      45|       12|       1|       0|       0|       0|        0|        2|        0|        0|  15|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 16:00:00|      0|         1|  12|   15|      49|        8|       1|       0|       0|       0|        0|        2|        0|        0|  16|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 17:00:00|      0|         1|  12|   15|      49|        7|       1|       0|       0|       0|        0|        2|        0|        0|  17|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 18:00:00|      0|         1|  10|   12|      56|       12|       1|       0|       0|       0|        0|        2|        0|        0|  18|     0|     0| 20|2011|    1|\n",
      "|2011-01-20 19:00:00|      0|         1|  10|   11|      56|       22|       1|       0|       0|       0|        1|        0|        0|        0|  19|     0|     0| 20|2011|    1|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Split TRAINING DATASET datetime into meaning columns such as hour,day,month,year,etc\n",
    "df_train = df_train_new.withColumn('hour',hour(col(\"datetime\")))\\\n",
    "                       .withColumn('minute',minute(col(\"datetime\")))\\\n",
    "                       .withColumn('second',second(col(\"datetime\")))\\\n",
    "                       .withColumn('day',dayofmonth(col(\"datetime\")))\\\n",
    "                       .withColumn('year',year(col(\"datetime\")))\\\n",
    "                       .withColumn('month',month(col(\"datetime\")))\n",
    "df_train.show()\n",
    "\n",
    "##Split TESTING DATASET datetime into meaning columns such as hour,day,month,year,etc\n",
    "df_test = df_test_new.withColumn('hour',hour(col(\"datetime\")))\\\n",
    "                       .withColumn('minute',minute(col(\"datetime\")))\\\n",
    "                       .withColumn('second',second(col(\"datetime\")))\\\n",
    "                       .withColumn('day',dayofmonth(col(\"datetime\")))\\\n",
    "                       .withColumn('year',year(col(\"datetime\")))\\\n",
    "                       .withColumn('month',month(col(\"datetime\")))\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorAssembler\n",
    "assembler_train_f = VectorAssembler(inputCols=['season_1','season_2','season_3','season_4','weather_1','weather_2','weather_3','weather_4','temp','atemp','humidity','windspeed','hour','minute','second','day','year','month','holiday','workingday'],outputCol=\"features\")\n",
    "assembled_train_f = assembler_train_f.transform(df_train)\n",
    "\n",
    "(trainingData, testData) = assembled_train_f.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count()\n",
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+--------------------+\n",
      "|           datetime|holiday|workingday|temp|atemp|humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|minute|second|day|year|month|            features|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+--------------------+\n",
      "|2011-01-01 01:00:00|      0|         0|   9|   13|      80|        0|     8|        32|   40|       1|       0|       0|       0|        1|        0|        0|        0|   1|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 02:00:00|      0|         0|   9|   13|      80|        0|     5|        27|   32|       1|       0|       0|       0|        1|        0|        0|        0|   2|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 03:00:00|      0|         0|   9|   14|      75|        0|     3|        10|   13|       1|       0|       0|       0|        1|        0|        0|        0|   3|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 04:00:00|      0|         0|   9|   14|      75|        0|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|   4|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 06:00:00|      0|         0|   9|   13|      80|        0|     2|         0|    2|       1|       0|       0|       0|        1|        0|        0|        0|   6|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 07:00:00|      0|         0|   8|   12|      86|        0|     1|         2|    3|       1|       0|       0|       0|        1|        0|        0|        0|   7|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 08:00:00|      0|         0|   9|   14|      75|        0|     1|         7|    8|       1|       0|       0|       0|        1|        0|        0|        0|   8|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 09:00:00|      0|         0|  13|   17|      76|        0|     8|         6|   14|       1|       0|       0|       0|        1|        0|        0|        0|   9|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 10:00:00|      0|         0|  15|   19|      76|       16|    12|        24|   36|       1|       0|       0|       0|        1|        0|        0|        0|  10|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 11:00:00|      0|         0|  14|   16|      81|       19|    26|        30|   56|       1|       0|       0|       0|        1|        0|        0|        0|  11|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 12:00:00|      0|         0|  17|   21|      77|       19|    29|        55|   84|       1|       0|       0|       0|        1|        0|        0|        0|  12|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 13:00:00|      0|         0|  18|   22|      72|       19|    47|        47|   94|       1|       0|       0|       0|        0|        2|        0|        0|  13|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-01 14:00:00|      0|         0|  18|   22|      72|       19|    35|        71|  106|       1|       0|       0|       0|        0|        2|        0|        0|  14|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-01 15:00:00|      0|         0|  18|   21|      77|       19|    40|        70|  110|       1|       0|       0|       0|        0|        2|        0|        0|  15|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-01 16:00:00|      0|         0|  17|   21|      82|       19|    41|        52|   93|       1|       0|       0|       0|        0|        2|        0|        0|  16|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-01 17:00:00|      0|         0|  18|   21|      82|       19|    15|        52|   67|       1|       0|       0|       0|        0|        2|        0|        0|  17|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-01 18:00:00|      0|         0|  17|   21|      88|       16|     9|        26|   35|       1|       0|       0|       0|        0|        0|        3|        0|  18|     0|     0|  1|2011|    1|(20,[0,6,8,9,10,1...|\n",
      "|2011-01-01 19:00:00|      0|         0|  17|   21|      88|       16|     6|        31|   37|       1|       0|       0|       0|        0|        0|        3|        0|  19|     0|     0|  1|2011|    1|(20,[0,6,8,9,10,1...|\n",
      "|2011-01-01 20:00:00|      0|         0|  16|   20|      87|       16|    11|        25|   36|       1|       0|       0|       0|        0|        2|        0|        0|  20|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-01 21:00:00|      0|         0|  16|   20|      87|       12|     3|        31|   34|       1|       0|       0|       0|        0|        2|        0|        0|  21|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+--------------------+\n",
      "|           datetime|holiday|workingday|temp|atemp|humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|minute|second|day|year|month|            features|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+--------------------+\n",
      "|2011-01-01 00:00:00|      0|         0|   9|   14|      81|        0|     3|        13|   16|       1|       0|       0|       0|        1|        0|        0|        0|   0|     0|     0|  1|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-01 05:00:00|      0|         0|   9|   12|      75|        6|     0|         1|    1|       1|       0|       0|       0|        0|        2|        0|        0|   5|     0|     0|  1|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-02 09:00:00|      0|         0|  15|   19|      76|       15|     1|        19|   20|       1|       0|       0|       0|        0|        2|        0|        0|   9|     0|     0|  2|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-02 16:00:00|      0|         0|  13|   16|      71|       11|     9|        67|   76|       1|       0|       0|       0|        0|        0|        3|        0|  16|     0|     0|  2|2011|    1|(20,[0,6,8,9,10,1...|\n",
      "|2011-01-02 19:00:00|      0|         0|  13|   14|      42|       30|     1|        29|   30|       1|       0|       0|       0|        1|        0|        0|        0|  19|     0|     0|  2|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-04 00:00:00|      0|         1|   6|    9|      55|        7|     0|         5|    5|       1|       0|       0|       0|        1|        0|        0|        0|   0|     0|     0|  4|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-04 02:00:00|      0|         1|   5|    7|      63|        8|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|   2|     0|     0|  4|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-04 13:00:00|      0|         1|   9|   11|      56|       12|    18|        79|   97|       1|       0|       0|       0|        1|        0|        0|        0|  13|     0|     0|  4|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-04 15:00:00|      0|         1|  11|   13|      52|       16|    17|        48|   65|       1|       0|       0|       0|        1|        0|        0|        0|  15|     0|     0|  4|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-04 17:00:00|      0|         1|  11|   13|      48|       15|    10|       202|  212|       1|       0|       0|       0|        1|        0|        0|        0|  17|     0|     0|  4|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-05 09:00:00|      0|         1|   9|    9|      37|       22|     6|       109|  115|       1|       0|       0|       0|        1|        0|        0|        0|   9|     0|     0|  5|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-05 11:00:00|      0|         1|  10|   11|      33|       22|    12|        34|   46|       1|       0|       0|       0|        1|        0|        0|        0|  11|     0|     0|  5|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-06 11:00:00|      0|         1|   9|   12|      44|        6|     2|        57|   59|       1|       0|       0|       0|        1|        0|        0|        0|  11|     0|     0|  6|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-06 12:00:00|      0|         1|  10|   14|      35|        0|     6|        78|   84|       1|       0|       0|       0|        1|        0|        0|        0|  12|     0|     0|  6|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-06 21:00:00|      0|         1|   9|   10|      55|       15|     0|        48|   48|       1|       0|       0|       0|        0|        2|        0|        0|  21|     0|     0|  6|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-08 13:00:00|      0|         0|   8|    9|      44|       22|     7|        95|  102|       1|       0|       0|       0|        1|        0|        0|        0|  13|     0|     0|  8|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-08 14:00:00|      0|         0|   8|    8|      32|       32|    12|        83|   95|       1|       0|       0|       0|        1|        0|        0|        0|  14|     0|     0|  8|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-09 18:00:00|      0|         0|   6|    6|      40|       22|     4|        44|   48|       1|       0|       0|       0|        1|        0|        0|        0|  18|     0|     0|  9|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "|2011-01-10 12:00:00|      0|         1|   8|    9|      40|       19|     5|        47|   52|       1|       0|       0|       0|        0|        2|        0|        0|  12|     0|     0| 10|2011|    1|(20,[0,5,8,9,10,1...|\n",
      "|2011-01-10 21:00:00|      0|         1|   5|    6|      59|       12|     1|        37|   38|       1|       0|       0|       0|        1|        0|        0|        0|  21|     0|     0| 10|2011|    1|(20,[0,4,8,9,10,1...|\n",
      "+-------------------+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+------+------+---+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()\n",
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='count')\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 140.253173\n",
      "r2: 0.400700\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        prediction|count|\n",
      "+------------------+-----+\n",
      "| -98.7184679057682|   16|\n",
      "|-45.69496224165778|    1|\n",
      "| 40.60745651673642|   20|\n",
      "| 46.79084519753815|   76|\n",
      "|160.11800633199164|   30|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.368014\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lrModel.transform(testData)\n",
    "lr_predictions.select(\"prediction\",\"count\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3240.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.regression.InternalLinearRegressionModelWriter.write(LinearRegression.scala:721)\r\n\tat org.apache.spark.ml.util.GeneralMLWriter.saveImpl(ReadWrite.scala:260)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 168.0 failed 1 times, most recent failure: Lost task 0.0 in stage 168.0 (TID 162, 192.168.2.14, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\SP.000\\Documents\\Edureka\\Machine Learning Engineer\\Python Spark Certification Training using PySpark\\Project\\trained_model\\linearregression.model\\metadata\\_temporary\\0\\_temporary\\attempt_20210116213211_0841_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2152)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 51 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\SP.000\\Documents\\Edureka\\Machine Learning Engineer\\Python Spark Certification Training using PySpark\\Project\\trained_model\\linearregression.model\\metadata\\_temporary\\0\\_temporary\\attempt_20210116213211_0841_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-8d9f297defab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlrModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trained_model/linearregression.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3240.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.regression.InternalLinearRegressionModelWriter.write(LinearRegression.scala:721)\r\n\tat org.apache.spark.ml.util.GeneralMLWriter.saveImpl(ReadWrite.scala:260)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 168.0 failed 1 times, most recent failure: Lost task 0.0 in stage 168.0 (TID 162, 192.168.2.14, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\SP.000\\Documents\\Edureka\\Machine Learning Engineer\\Python Spark Certification Training using PySpark\\Project\\trained_model\\linearregression.model\\metadata\\_temporary\\0\\_temporary\\attempt_20210116213211_0841_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2152)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 51 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\SP.000\\Documents\\Edureka\\Machine Learning Engineer\\Python Spark Certification Training using PySpark\\Project\\trained_model\\linearregression.model\\metadata\\_temporary\\0\\_temporary\\attempt_20210116213211_0841_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "lrModel.write().overwrite().save(\"trained_model/linearregression.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(20, {0: 1.0, 4: 1.0, 8: 10.0, 9: 11.0, 10: 56.0, 11: 26.0, 15: 20.0, 16: 2011.0, 17: 1.0, 19: 1.0}), prediction=-32.43047899598605),\n",
       " Row(features=SparseVector(20, {0: 1.0, 4: 1.0, 8: 10.0, 9: 13.0, 10: 56.0, 12: 1.0, 15: 20.0, 16: 2011.0, 17: 1.0, 19: 1.0}), prediction=-34.97443831744022),\n",
       " Row(features=SparseVector(20, {0: 1.0, 4: 1.0, 8: 10.0, 9: 13.0, 10: 56.0, 12: 2.0, 15: 20.0, 16: 2011.0, 17: 1.0, 19: 1.0}), prediction=-27.438666224043118),\n",
       " Row(features=SparseVector(20, {0: 1.0, 4: 1.0, 8: 10.0, 9: 12.0, 10: 56.0, 11: 11.0, 12: 3.0, 15: 20.0, 16: 2011.0, 17: 1.0, 19: 1.0}), prediction=-16.143902212847024),\n",
       " Row(features=SparseVector(20, {0: 1.0, 4: 1.0, 8: 10.0, 9: 12.0, 10: 56.0, 11: 11.0, 12: 4.0, 15: 20.0, 16: 2011.0, 17: 1.0, 19: 1.0}), prediction=-8.60813011942082)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
