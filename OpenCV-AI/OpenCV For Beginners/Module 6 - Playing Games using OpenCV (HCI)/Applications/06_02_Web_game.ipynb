{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Control games with face position\n",
    "In this notebook, you will learn how to connect the dots between using OpenCV to identify faces within video feeds, and connecting that to PyAutoGUI to drive keyboard inputs in different video game applications. Be sure to test and tinker with the accompanying python script along side this notebook to run the end to end application, as you may not be able to effectively run it from within the notebook itself.\n",
    "\n",
    "The high level steps look like the following:\n",
    "1. Define a webcam capture loop, drawing the \"center bounds\".\n",
    "2. Load and use a pre-made face detection model.\n",
    "3. For any faces found, determine if has moved outside the center-position bounds.\n",
    "4. If out of bounds, send the according key press to the operating system.\n",
    "5. Finally, we can run the script and switch to a window with a game to control.\n",
    "\n",
    "You can see the example output below of what we are going to achieve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Output/Output_Pac_man.mov\" controls  width=\"800\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video('Output/Output_Pac_man.mov',  width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui as gui\n",
    "import time\n",
    "\n",
    "# Set keypress delay to 0.\n",
    "gui.PAUSE = 0\n",
    "\n",
    "# Loading the pre-trained face model.\n",
    "model_path = './model/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "prototxt_path = './model/deploy.prototxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Webcam loop and drawing the \"center bounds\"\n",
    "\n",
    "To get started, we will create the basic structure for our main gameplay loop. Notice there are placeholder comments for now for the functions that our loop will need to call, which we will add later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(prototxt_path, model_path):\n",
    "    '''\n",
    "    Run the main loop until cancelled.\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Getting the Frame width and height.\n",
    "    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "    # Co-ordinates of the bounding box on frame\n",
    "    left_x, top_y = frame_width // 2 - 150, frame_height // 2 - 200\n",
    "    right_x, bottom_y = frame_width // 2 + 150, frame_height // 2 + 200\n",
    "    bbox = [left_x, right_x, bottom_y, top_y]\n",
    "\n",
    "    while not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            return 0\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # To be added: Detecting and drawing bounding box around faces\n",
    "\n",
    "        # Drawing the control rectangle in the center of the frame.\n",
    "        frame = cv2.rectangle(\n",
    "            frame, (left_x, top_y), (right_x, bottom_y), (0, 0, 255), 5)\n",
    "        # To be added: Checking for game-start position, and checking to run keyboard press.\n",
    "        # Exit the loop on pressing the `esc` key.\n",
    "        k = cv2.waitKey(5)\n",
    "        if k == 27:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using a pre-made face detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our pre-trained face model, we will need to do the following steps:\n",
    "\n",
    "1. Load in the deep neural network (DNN).\n",
    "2. Transform an input frame into the require format \n",
    "3. Set this frame as the input to the face detection model.\n",
    "4. Read out any detected results.\n",
    "\n",
    "More details about how face detection works is included in a later module, in the meantime you can view the key functions we are using for now.\n",
    "\n",
    "<hr   style=\"border:none; height: 4px; background-color: #D3D3D3 \" />\n",
    "\n",
    "## Reading the DNN model\n",
    "\n",
    "**`readNetFromCaffe()`** Reads a network model stored in [Caffe](http://caffe.berkeleyvision.org/) framework's format.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Function Syntax </font>\n",
    "``` python\n",
    "cv2.dnn.readNetFromCaffe( prototxt[, caffeModel] )\n",
    "```\n",
    "The function has **2 required arguments** and 1 optional:\n",
    "\n",
    "1. `prototxt` path to the .prototxt file with text description of the network architecture.\n",
    "2. `caffeModel`\tpath to the .caffemodel file with learned network.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">OpenCV Documentation</font>\n",
    "\n",
    "[**`cv2.dnn.readNetFromCaffe()`**](https://docs.opencv.org/4.5.2/d6/d0f/group__dnn.html#ga29d0ea5e52b1d1a6c2681e3f7d68473a)\n",
    "\n",
    "## Converting an image into the model formal\n",
    "\n",
    "**`blobFromImage()`** Creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scales values by scalefactor and swap Blue and Red channels.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Function Syntax </font>\n",
    "``` python\n",
    "dst = cv2.dnn.blobFromImage( image[, scalefactor[, size[, mean[, swapRB[, crop[, ddepth]]]]]] )\n",
    "```\n",
    "`dst`: Is the output image of the same size and depth as `image`.\n",
    "\n",
    "The function has **1 required arguments** and rest are optional:\n",
    "\n",
    "1. `image` is the input image (with 1-, 3- or 4-channels).\n",
    "2. `scalefactor` is the multiplier for image values.\n",
    "3. `size`is  the spatial size for output image.\n",
    "4. `mean` scalar with mean values which are subtracted from channels. Values are intended to be in (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true. \n",
    "5. `swapRB` is the flag which indicates that swap first and last channels in 3-channel image is necessary.\n",
    "6. `crop` flag which indicates whether image will be cropped after resize or not\n",
    "7. `ddepth`\tdepth of output blob. Choose either CV_32F or CV_8U.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">OpenCV Documentation</font>\n",
    "\n",
    "[**`cv2.dnn.blobFromImage()`**](https://docs.opencv.org/4.5.2/d6/d0f/group__dnn.html#ga98113a886b1d1fe0b38a8eef39ffaaa0)\n",
    "\n",
    "\n",
    "\n",
    "## Setting input value\n",
    "\n",
    "**`setInput()`** Sets the new input value for the network.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Function Syntax </font>\n",
    "``` python\n",
    "cv2.dnn_Net.setInput( blob[, name[, scalefactor[, mean]]] )\n",
    "```\n",
    "The function has **1 required arguments** and 3 optional:\n",
    "\n",
    "1. `blob` is a new blob.\n",
    "2. `name` is the name of input layer.\n",
    "3. `scalefactor` is an optional normalization scale.\n",
    "4. `mean` is an optional mean subtraction values.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">OpenCV Documentation</font>\n",
    "\n",
    "[**`cv2.dnn_Net.setInput()`**](https://docs.opencv.org/4.5.2/db/d30/classcv_1_1dnn_1_1Net.html#a5e74adacffd6aa53d56046581de7fcbd)\n",
    "\n",
    "\n",
    "## Detections using the DNN Model\n",
    "\n",
    "**`forward()`** Runs forward pass to compute output of layer with name outputName. Returns blob for first output of specified layer.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Function Syntax </font>\n",
    "``` python\n",
    "cv2.dnn_Net.forward( [, outputName] )\n",
    "```\n",
    "The function has 1 optional arguments:\n",
    "\n",
    "1. `outputName`\tis the name of layer for which output is needed.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">OpenCV Documentation</font>\n",
    "\n",
    "[**`cv2.dnn_Net.forward()`**](https://docs.opencv.org/4.5.2/db/d30/classcv_1_1dnn_1_1Net.html#a98ed94cb6ef7063d3697259566da310b)\n",
    "\n",
    "<hr   style=\"border:none; height: 4px; background-color: #D3D3D3 \" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style='color:rgb(50,120,230)'> 2.1 Read the neural network model in main function</font>\n",
    "\n",
    "We will add the following line one at the top of our main `play` function, but outside the loop, so that we only load the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style='color:rgb(50,120,230)'> 2.2 Function to detect the faces in the frame. </font>\n",
    "\n",
    "We then need to define the function which runs the detection of faces. We transform the image into blob format with `cv2.dnn.blobFromImage`, assign it as an input into the model using `net.setInput`, and then run the detections using `net.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(net, frame):\n",
    "    '''\n",
    "    Detect the faces in the frame.\n",
    "\n",
    "    returns: list of faces in the frame\n",
    "                here each face is a dictionary of format-\n",
    "                {'start': (startX,startY), 'end': (endX,endY), 'confidence': confidence}\n",
    "    '''\n",
    "    detected_faces = []\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)),\n",
    "        1.0,\n",
    "        (300, 300),\n",
    "        (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            detected_faces.append({\n",
    "                'start': (startX, startY),\n",
    "                'end': (endX, endY),\n",
    "                'confidence': confidence})\n",
    "    return detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style='color:rgb(50,120,230)'> 2.3 Function to draw rectangular bounding box around detected faces.</font>\n",
    "\n",
    "Finally, we want to visually draw a rectangle for each detected face on the screen. This is regardless of whether or not a keyboard signal is to be sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFace(frame, detected_faces):\n",
    "    '''\n",
    "    Draw rectangular box over detected faces.\n",
    "\n",
    "    returns: frame with rectangular boxes over detected faces.\n",
    "    '''\n",
    "    for face in detected_faces:\n",
    "        cv2.rectangle(frame, face['start'], face['end'], (0, 255, 0), 10)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detect movement outside the center box\n",
    "\n",
    "This is a function to check that a detected face is inside the bounding box at the center of the frame. If this value is True on one frame and False on the next, then it will tell a future function that a keyboard press should occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRect(detected_faces, bbox):\n",
    "    '''\n",
    "    Check for a detected face inside the bounding box at the center of the frame.\n",
    "\n",
    "    returns: True or False.\n",
    "    '''\n",
    "    for face in detected_faces:\n",
    "        x1, y1 = face['start']\n",
    "        x2, y2 = face['end']\n",
    "        if x1 > bbox[0] and x2 < bbox[1]:\n",
    "            if y1 > bbox[3] and y2 < bbox[2]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Send keyboard press on detected movement\n",
    "\n",
    "Based on the output of the `checkRect` function, we can now decide whether to send a keyboard arrow press event to the operating system via PyAutoGUI (imported as `gui`). The `last_mov` check is added to make sure the character doesnt keep drifting in the previous detection.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='border:none; height: 4px; background-color:#D3D3D3'/>\n",
    "\n",
    "## Press a button\n",
    "\n",
    "`press()`  function is really just a wrapper for the `keyDown()` and `keyUp()` functions, which simulate pressing a key down and then releasing it up.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Function Syntax </font>\n",
    "``` python\n",
    "pyautogui.press( key )\t\n",
    "```\n",
    "\n",
    "The function has **1 required input argument**:\n",
    "\n",
    "1. `key` string of the key to be pressed from the [pyautogui.KEYBOARD_KEYS](https://pyautogui.readthedocs.io/en/latest/keyboard.html#keyboard-keys)\n",
    "\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">PyAutoGUI Documentation</font>\n",
    "\n",
    "[**`press()`**](https://pyautogui.readthedocs.io/en/latest/keyboard.html)\n",
    "\n",
    "<hr style='border:none; height: 4px; background-color:#D3D3D3'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(detected_faces, bbox):\n",
    "    '''\n",
    "    Press correct button depending on the position of detected face and bbox.\n",
    "\n",
    "    The last_mov check is added for making sure the character doesn't keep\n",
    "    drifting in the previous detection.\n",
    "    '''\n",
    "    global last_mov\n",
    "    for face in detected_faces:\n",
    "        x1, y1 = face['start']\n",
    "        x2, y2 = face['end']\n",
    "\n",
    "        # Center\n",
    "        if checkRect(detected_faces, bbox):\n",
    "            last_mov = 'center'\n",
    "            return\n",
    "\n",
    "        elif last_mov == 'center':\n",
    "            # Left\n",
    "            if x1 < bbox[0]:\n",
    "                gui.press('left')\n",
    "                last_mov = 'left'\n",
    "            # Right\n",
    "            elif x2 > bbox[1]:\n",
    "                gui.press('right')\n",
    "                last_mov = 'right'\n",
    "            # Down\n",
    "            if y2 > bbox[2]:\n",
    "                gui.press('down')\n",
    "                last_mov = 'down'\n",
    "            # Up\n",
    "            elif y1 < bbox[3]:\n",
    "                gui.press('up')\n",
    "                last_mov = 'up'\n",
    "\n",
    "            # Print out the button pressed if any.\n",
    "            if last_mov != 'center':\n",
    "                print(last_mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Updating the play function\n",
    "\n",
    "We can now update our `play` loop to call the functions defined in the prior steps. Below, we have added the calls to detect faces, draw them on the video feed, and then send the according commands to PyAutoGUI for keyboard actions. Notice the loop below is also enhanced further with an FPS display, calculated manually using the time elapsed between displayed frames.\n",
    "\n",
    "To increase performance, the loop captures and process every other frame for face detection and positioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(prototxt_path, model_path):\n",
    "    '''\n",
    "    Run the main loop until cancelled.\n",
    "    '''\n",
    "    global last_mov\n",
    "    # Used to record the time when we processed last frame.\n",
    "    prev_frame_time = 0\n",
    "    # Used to record the time at which we processed current frame.\n",
    "    new_frame_time = 0\n",
    "\n",
    "    net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Counter for skipping frame.\n",
    "    count = 0\n",
    "\n",
    "    # Used to initialize the game.\n",
    "    init = 0\n",
    "\n",
    "    # Getting the Frame width and height.\n",
    "    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "    # Co-ordinates of the bounding box on frame\n",
    "    left_x, top_y = frame_width // 2 - 150, frame_height // 2 - 200\n",
    "    right_x, bottom_y = frame_width // 2 + 150, frame_height // 2 + 200\n",
    "    bbox = [left_x, right_x, bottom_y, top_y]\n",
    "\n",
    "    while not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        fps = 0\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            return 0\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # Detect the face.\n",
    "        detected_faces = detect(net, frame)\n",
    "        # Draw bounding box around detected faces.\n",
    "        frame = drawFace(frame, detected_faces)\n",
    "        # Drawing the control rectangle in the center of the frame.\n",
    "        frame = cv2.rectangle(\n",
    "            frame, (left_x, top_y), (right_x, bottom_y), (0, 0, 255), 5)\n",
    "\n",
    "        # Skipping every alternate frame.\n",
    "        if count % 2 == 0:\n",
    "            # For first pass.\n",
    "            if init == 0:\n",
    "                # If face is inside the control rectangle.\n",
    "                if checkRect(detected_faces, bbox):\n",
    "                    init = 1\n",
    "                    cv2.putText(\n",
    "                        frame, 'Game is running', (100, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.waitKey(10)\n",
    "                    last_mov = 'center'\n",
    "                    # Click to start the game.\n",
    "                    gui.click(x=500, y=500)\n",
    "            else:\n",
    "\n",
    "                move(detected_faces, bbox)\n",
    "                cv2.waitKey(50)\n",
    "        # Calculating the FPS.\n",
    "        new_frame_time = time.time()\n",
    "        fps = int(1 / (new_frame_time - prev_frame_time))\n",
    "        prev_frame_time = new_frame_time\n",
    "\n",
    "        frame = cv2.putText(\n",
    "            frame, str(fps) + 'FPS', (200, 100),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "        cv2.imshow('camera_feed', frame)\n",
    "        count += 1\n",
    "\n",
    "        # Exit the loop on pressing the `esc` key.\n",
    "        k = cv2.waitKey(5)\n",
    "        if k == 27:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style='color:rgb(50,120,230)'> Calling the main loop </font>\n",
    "\n",
    "Finally, we call our `play` function, which will run until the escape key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\n"
     ]
    }
   ],
   "source": [
    "# Used to pass the previous move of the user to the play() function.\n",
    "last_mov = ''\n",
    "# play(prototxt_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can run `play()` by uncommenting it or use the python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examples with different games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Output/Output_Web_Game.mov\" controls  width=\"900\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('Output/Output_Web_Game.mov',  width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we are using PyAutoGUI to control the keyboard, we can play any game by slight modification to the logic of the `move()` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pac Man\n",
    "\n",
    "You can play [Google's Pacman Doodle](https://www.google.com/logos/2010/pacman10-i.html) entirely with up, down, left, and right inputs too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Output/Output_Pac_man.mov\" controls  width=\"900\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('Output/Output_Pac_man.mov',  width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racing Game\n",
    "\n",
    "In this example racing game, only the left-right control inputs are controlled by the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Output/Output_car_race.mov\" controls  width=\"900\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('Output/Output_car_race.mov',  width = 900)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e654b3bc3aace0335b326231d51e90ebd214a7f2d0629a648660f7deb4b3382"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
