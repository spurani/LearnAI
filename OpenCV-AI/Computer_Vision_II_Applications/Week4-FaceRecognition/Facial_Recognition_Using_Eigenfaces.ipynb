{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style = \"color:rgb(50,120,229)\">Facial Recognition Using Eigenfaces<\/font>\n",
                "In this module, we will discuss how we can use basic mathematical concepts like eigen vectors to perform Face Recognition. \n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">Mathematical Background<\/font>\n",
                "\n",
                "Before we move forward, we need to arm ourselves with the some basics. There is some linear algebra involved, but we will try to make it as simple as possible. We will explain some concepts first in the abstract, but trust me it will all come together in the end. \n",
                "\n",
                "**<font style=\"color:rgb(255,0,0)\">Note:<\/font>** In this document bold font math symbols denote matrices and vectors. Regular font math symbols are scalars. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Eigenvectors and Eigenvalues<\/font>\n",
                "\n",
                "Let\u2019s say we have a nxn matrix $\\pmb A$. If we multiply this matrix with an nx1 vector $\\pmb u$, it will produce a new vector $\\pmb u'= \\pmb{Au}$. We say the matrix $\\pmb A$ has \"linearly transformed\" vector $\\pmb u$ to $\\pmb u'$. The new vector $\\pmb u'$ is also n x 1.\n",
                "\n",
                "| <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-ordinaryVector.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-ordinaryVector.png\"\/><\/a><\/center> | <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-eigenVector.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-eigenVector.png\"\/><\/a><\/center> | \n",
                "| -------- | -------- | \n",
                "| <center>(a)<\/center>     |<center>(b)<\/center>     |\n",
                "\n",
                "<center>Figure 1: A matrix $\\pmb A$ changes the magnitude and direction of an arbitrary vector $\\pmb u$. However, if the vector is the eigenvector $\\pmb v$ of the matrix, the matrix changes only its magnitude and not its direction.<\/center>\n",
                "\n",
                "Remember, a vector has a magnitude and a direction. In general, when it is transformed by a matrix ( i.e. we pre-multiply a matrix with it ), we get another vector with a different magnitude and direction. Figure 1 (a) shows this graphically. \n",
                "\n",
                "However, for a matrix certain vectors, called **eigenvectors** are special. An eigenvector of a matrix is a vector $\\pmb v$ such that when the matrix $\\pmb A$ is multiplied with it, the direction of the vector does not change; only the magnitude changes. In other words\n",
                "\n",
                "$$\n",
                "\\pmb{A v} = \\lambda \\pmb{v} \\hspace{12cm}(1)\n",
                "$$\n",
                "\n",
                "where , $\\lambda$ is a scalar and is called the **eigenvalue** corresponding to the eigenvector $\\pmb v$. Figure 1(b) depicts this graphically. \n",
                "\n",
                "Let us take a concrete example to understand this. Consider the following 3x3 matrix\n",
                "\\begin{align*}\n",
                "\\mathbf{A} =\n",
                "\\begin{bmatrix}\n",
                "1.0453 \\; 1.1066 \\; 0.8294 \\\\\n",
                "1.1066 \\; 1.4661 \\; 1.0992 \\\\\n",
                "0.8294 \\; 1.0992 \\; 0.8931\n",
                "\\end{bmatrix}\n",
                "\\end{align*}\n",
                "\n",
                "Consider a special vector $\\mathbf{v}$, where\n",
                "\n",
                "\\begin{align*}\n",
                "\\mathbf{v} =\n",
                "\\begin{bmatrix}\n",
                "-0.1233 \\\\\n",
                "0.6644 \\\\\n",
                "-0.7371\n",
                "\\end{bmatrix}\n",
                "\\end{align*}\n",
                "\n",
                "Let us multiply the matrix $\\mathbf{A}$ with the vector $\\mathbf{v}$.\n",
                "\n",
                "\\begin{align*}\n",
                "\\mathbf{Av}=\\begin{bmatrix}\n",
                "1.0453 \\; 1.1066 \\; 0.8294 \\\\\n",
                "1.1066 \\; 1.4661 \\; 1.0992 \\\\\n",
                "0.8294 \\; 1.0992 \\; 0.8931\n",
                "\\end{bmatrix}\n",
                "\\begin{bmatrix}\n",
                "-0.1233 \\\\\n",
                "0.6644 \\\\\n",
                "-0.7371\n",
                "\\end{bmatrix}\n",
                "\\; =\n",
                "\\begin{bmatrix}\n",
                "-0.0051 \\\\\n",
                "0.0274 \\\\\n",
                "-0.0304\n",
                "\\end{bmatrix}\n",
                "\\; \n",
                "\\end{align*}\n",
                "\\begin{align*}\n",
                "= 0.0412\n",
                "\\begin{bmatrix}\n",
                "-0.1233 \\\\\n",
                "0.6644 \\\\\n",
                "-0.7371\n",
                "\\end{bmatrix}\n",
                "\\; = \\lambda \\mathbf{v}\n",
                "\\end{align*}\n",
                "\n",
                "where $\\lambda = 0.0412$. Notice, when we multiplied the matrix $\\mathbf{A}$ to the vector $\\mathbf{v}$, it only changed the magnitude of the vector $\\mathbf{v}$ by $\\lambda$ but did not change its direction. There are only 3 directions ( including the $\\mathbf{v}$ in the example above ) for which the matrix $\\mathbf{A}$ will act like a scalar multiplier. These three directions are the Eigen vectors of the matrix and the scalar multipliers are the Eigen values.\n",
                "\n",
                "So, an Eigen vector $\\mathbf{v}$ of a matrix $\\mathbf{A}$ is a vector whose direction does not change when the matrix is multiplied to it. \n",
                "\n",
                "\n",
                "When an n x n matrix is full rank ( i.e. any row is not a linear combination of other rows ), there are n eigenvectors. \n",
                "\n",
                "You may be thinking eigenvectors are interesting, but what are they useful for? Eigenvectors are very useful in understanding the nature of matrices, but let us see a very practical use in the next section. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">How to find the Eigenvectors and Eigenvalues of a matrix?<\/font>\n",
                "\n",
                "There are a few different ways to solve for the eigenvalues. Here we will go over an easy to understand example. In practice though, any matrix library would have an implementation and you typically do not have to worry about the details. \n",
                "\n",
                "For finding the eigenvalues of a matrix, we set the determinant of the matrix $\\pmb A - \\lambda\\pmb I$ to 0. \n",
                "\n",
                "$$\n",
                "|\\pmb A - \\lambda\\pmb I|=0 \\hspace{11cm}(2)\n",
                "$$\n",
                "\n",
                "The above expression results in a quadratic equation when $\\pmb A$ is 2x2 and so we get two values of $\\lambda$ by solving the quadratic equation.  In general, we solve for a polynomial in $\\lambda$ and obtain n different values of $\\lambda$ for an nxn matrix . \n",
                "\n",
                "Once, the eigenvalues are solved, the eigenvectors can be solved by plugging in the values of $\\lambda$ in the original equation $\\pmb {Av}=\\lambda\\pmb v$. \n",
                "\n",
                "Wikipedia provides [an example](https:\/\/en.wikipedia.org\/wiki\/Eigenvalues_and_eigenvectors) for solving the eigenvalues and eigenvectors for a 2x2 matrix. \n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">Principal Component Analysis (PCA)<\/font>\n",
                "PCA is an important machine learning concept used for dimensionality reduction. But before we proceed, let us first understand the term **Variance**.\n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">What is variance?<\/font>\n",
                "The variance measures the spread of the data. In Figure 1 (a), the points have a high variance because they are spread out, but in Figure 1 (b), the points have a low variance because they are close together.\n",
                "\n",
                "<center>\n",
                "<a href=\"http:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/high-variance-versus-low-variance-data.png\"><img class=\"wp-image-6835 size-large\" src=\"http:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/high-variance-versus-low-variance-data-1024x509.png\" alt=\"High variance versus low variance\"\/><\/a> Figure 1. (a) Left : High Variance Data (b) Right : Low variance data\n",
                "<\/center>\n",
                "\n",
                "&nbsp;\n",
                "Also, note that in Figure 1 (a) the variance is not the same in all directions. The direction of maximum variance is especially important. Let's see why.\n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Why do we care about the direction of maximum variance?<\/font>\n",
                "<center>\n",
                "<a href=\"http:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/zero-variance-along-y-axis.png\"><img class=\"size-full wp-image-6839\" src=\"http:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/zero-variance-along-y-axis.png\" alt=\"Zero variance along y-axis\" width=\"400\" height=\"400\" \/><\/a> \n",
                "\n",
                "Figure 2 : Zero variance along the y-axis\n",
                "<\/center>\n",
                "\n",
                "Variance encodes information contained in the data. For example, if you had 2D data represented by points with $(x,y)$ coordinates. For n such points, you need 2n numbers to represent this data. Consider a special case where for every data point the value along the y-axis was 0 (or constant). This is shown in Figure 2\n",
                "\n",
                "It is fair to say that there is no (or very little) information along the y-axis. You can compactly represent this data using n numbers to represent its value along the x-axis and only 1 common number to represent the constant along the y-axis. Because there is more variance along the x-axis, there is more information, and hence we have to use more numbers to represent this data. On the other hand, since there is no variance along the y-axis, a single number can be used to represent all information contained in n points along this axis.\n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">What are Principal Components?<\/font>\n",
                "Let\u2019s say we have 2D data points as shown in Figure 1.\n",
                "\n",
                "<table>\n",
                "    <tr>\n",
                "        <th><center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-2Ddata.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-2Ddata.png\"\/><\/a><\/center><\/th>\n",
                "        <th><center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-principalComponent2Ddata.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w9-m1-principalComponent2Ddata.png\"\/><\/a><\/center><\/th>\n",
                "    <\/tr>\n",
                "    <tr>\n",
                "        <td><center>(a)<\/center><\/td>\n",
                "        <td><center>(b)<\/center><\/td>\n",
                "    <\/tr>\n",
                "    <tr>\n",
                "        <td colspan =\"2\"><center>Figure 2: (a) 2D data. (b) Principal components of the data.<\/center> <\/td>\n",
                "    <\/tr>\n",
                "<\/table>\n",
                "\n",
                "Suppose, we want to represent this 2D data using a single number. This is just another way of saying that we want to reduce the dimension of the data from 2D to 1D. Should we use the x-coordinate of the data or the y-coordinate ? Or is there a number that better captures the distribution of data ?\n",
                "\n",
                "In Figure 2 (b) we can see the data varies the most along the blue line. If we were to use a single number to represent our data, we are better off using the projection of the points onto the blue line instead of either the x or the y axis. The blue and the green lines are called the principal components. \n",
                "\n",
                "The first principal component is the direction of maximum variance. The second principal component is the direction of maximum variance in a direction perpendicular to the first principal component. In 2D there is just one such direction. In 3D we have a plane that is perpendicular to the first principal component and so the second principal component is the direction of maximum variance on this plane. \n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">How to perform PCA?<\/font>\n",
                "\n",
                "Let\u2019s say we have several data points and we want to find the principal components of this data. A data point in an n-dimensional is nothing but a nx1 vector. \n",
                "\n",
                "Here are the steps. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Step 1:  Calculate the mean vector<\/font>\n",
                "\n",
                "We simply add all the data points and divide the result by the number of data points. The mean is also an nx1 vector. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Step 2: Subtract the mean vector<\/font> \n",
                "\n",
                "From each data point we subtract the mean. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Step 3: Create a data matrix<\/font>\n",
                "\n",
                "We next create a matrix where each column is the mean-subtracted data point. Let\u2019s call it $\\pmb X$. If there are m data points, $\\pmb X$ is n x m in size.\n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Step 4: Calculate the covariance matrix<\/font>\n",
                "\n",
                "The covariance matrix captures the information about the spread of the data. For a data matrix $\\pmb{X}$, the covariance matrix is simply given by \n",
                "\n",
                "$$\n",
                "C = \\frac{1}{m} \\pmb{X}\\pmb{X}^T\\hspace{11cm}(3)\n",
                "$$\n",
                "\n",
                "\n",
                "The covariance matrix has the following properties. \n",
                "\n",
                "1. The diagonal elements measure the variance of data along the axes. \n",
                "\n",
                "2. The off-diagonal elements measure the correlation of data along the different axes. \n",
                "\n",
                "The figure below illustrates how the covariance matrix changes depending on the spread of data in different directions ( for a 2-dimensional data ).\n",
                "\n",
                "<center>\n",
                "<a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/covariance-matrix.png\"><img class=\"wp-image-6915\" src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/covariance-matrix.png\" alt=\"Covariance Matrix\" width=\"700\" height=\"282\" \/><\/a> \n",
                "<\/center>\n",
                "\n",
                "Figure 5: Left : When the data is evenly spread in all directions, the covariance matrix has equal diagonal elements and zero off-diagonal elements. Center: When the data spread is elongated along one of the axes, the diagonal elements are unequal, but the off diagonal elements are zero. Right : In general the covariance matrix has both diagonal and off -diagonal elements.\n",
                "\n",
                "For example, if we have two dimensional data. Along one axis we have temperature of San Diego on a particular day, and on the other axis we have the price of NVIDIA stock. Temperature of San Diego will have a low variance and the NVIDIA stock will have high variance, and the diagonal elements of the covariance matrix will reflect that fact. On the other hand, NVIDIA stock price has nothing to do with San Diego\u2019s temperature and so the off-diagonal elements will be small. On the other hand, if our data consists of NVIDIA stock price on one axis and popularity of Gaming or AI on the other axis, we can expect high off-diagonal values. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Step 5: Find the principal components<\/font>\n",
                "\n",
                "The principal components are the Eigen vectors of the covariance matrix. The first principal component is the Eigen vector corresponding to the largest Eigen value, the second principal component is the Eigen vector corresponding to the second largest Eigen value and so on and so forth.\n",
                " \n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">Eigenfaces<\/font>\n",
                "\n",
                "In this section, we will learn how PCA can be used for face recognition in a step by step manner. This method is popularly called Eigenfaces. While you will never use Eigenfaces in a state of the art Face Recognition application today ( because much better techniques exist ), but you will learn a few important concepts of Computer Vision and get a general idea about how to think about face recognition. If you want to build a non-critical application ( e.g. Doppelganger or Celebrity Double app ), you can definitely use Eigenfaces. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Face Detection and Normalization<\/font>\n",
                "\n",
                "The first step of all face recognition systems is face detection. The detected face is cropped and resized to fixed size. Usually, the images are converted to grayscale. For simplicity, we will assume the face to be square of size NxN after resizing.\n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Face as a vector<\/font>\n",
                "\n",
                "An NxN facial image can also be thought as an $N^2$ x 1 vector by simply unraveling the NxN matrix into a tall vector ( either row-wise or column-wise ). As we have seen before, any vector can also be thought as a point in higher dimensional space. \n",
                "\n",
                "In an ideal world, all image of a person\u2019s face when represented as a point in this  $N^2$ x 1 dimensional space will cluster together. And the images of a different person will cluster together in a different region of this $N^2$x1 dimensional space. The problem is that this space is huge. For a 100x100 grayscale image of a face, the space is 10,000 dimensional! We can reduce the dimension of this space using PCA while retaining much of the essential information. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">What are Eigenfaces?<\/font>\n",
                "\n",
                "In the previous section, we saw how we can find the principal components analysis to reduce the dimensionality of data. We can do the same for the face vectors obtained in the previous section. However, the size of the covariance matrix can be very large. E.g. a 100x100 image results in a 10,000 element long vector. The covariance matrix for this kind of data would be 10,000 x 10,000  and finding the eigenvalues and eigenvectors of this matrix is computationally very expensive. \n",
                "\n",
                "Fortunately, the size of the data matrix is limited by the number of images (M) in the dataset. If there are 100 images in the dataset, our data matrix is 10,000 x 100. A new matrix can be defined as\n",
                "\n",
                "$$\n",
                "C' = \\frac{1}{M}\\pmb{X}^T\\pmb{X}\n",
                "$$\n",
                "\n",
                "Notice the difference between $\\pmb C'$ from the original matrix $\\pmb C$ given by \n",
                "\n",
                "$$\n",
                "\\pmb{C} = \\frac{1}{M} \\pmb{X} \\pmb{X}^T\n",
                "$$\n",
                "\n",
                "$\\pmb C'$ is only MxM ( e.g. 100x100 )  in size whereas $\\pmb C$ is $N^2$ x $N^2$  ( 10,000 x 10,000) in size. Mathematically it can be shown the eigenvalues of the new matrix $\\pmb C'$ are the same as the top M eigenvalues of the original matrix $\\pmb C$. In addition, the eigenvectors $\\pmb v_i$ of the original matrix are related to the eigenvectors  $\\pmb {v_i}'$ of the new matrix by \n",
                "\n",
                "$$\n",
                "\\pmb{v_i} = \\pmb{X v'_i}\n",
                "$$\n",
                "\n",
                "This greatly simplifies calculation. The vectors are normalized ( i.e. $\\pmb v_i$ is divided by $||\\pmb v_i||$ ). \n",
                "\n",
                "These eigenvectors ( $\\pmb v_i$ ) are called **eigenfaces**.\n",
                "\n",
                "You can have a look at **[this tutorial](https:\/\/www.learnopencv.com\/eigenface-using-opencv-c-python\/)** on how to find eigen faces using PCA.\n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">Eigenfaces for Face Recognition<\/font>\n",
                "\n",
                "Once the eigenvectors are calculated the top K eigenvectors are chosen to represent every face. Every mean subtracted face in the training set can now be represented using these eigenfaces by \n",
                "\n",
                "$$\n",
                "\\pmb{X}_i = \\sum^{K}_{k=1} w_k \\pmb{v}_k\n",
                "$$  \n",
                "\n",
                "Where $w_k$ is just the projection of the face $\\pmb X_i$ onto the eigenvector $\\pmb v_k$ ( i.e. $w_k={v_k}^TX_i$ ). Once this is done the image is identified using the weight vector $\\pmb w$. \n",
                "\n",
                "You may be thinking we had earlier represented the face using an $N^2$ x 1 vector and now we are representing it by a vector $\\pmb w$ of length K. What is this for? \n",
                "\n",
                "The short answer is that this new space is a more compact representation of the face. In this space, the hope is that two images of the same person are more likely to be clustered together than in the old space. \n",
                "\n",
                "**<font style=\"color:rgb(255,0,0)\">Note:<\/font>** Did I use the word \"hope\"? Yes, because it is not obvious why reducing the dimensionality of the space would improve classification \/ recognition. In fact, in other classification problems, using PCA can actually hurt. I have included this note to make a philosophical point that experimentation and trial and error are hugely important tools because the concepts that solve a problem in one domain can fall flat in other domains depending on kind of data. \n",
                "\n",
                "Given a face of a person it can now be presented in terms of these K eigenvectors using the following steps. \n",
                "\n",
                "1. Resize and crop the face to NxN. \n",
                "\n",
                "2. Subtract the mean. \n",
                "\n",
                "3. Vectorize the image to $N^2$x1\n",
                "\n",
                "4. Calculate a weight vector for the new face. \n",
                "\n",
                "5. Compare this weight vector to all faces in the training set. The identity of the person in the training image that is closest to this image is returned as a match. \n",
                "\n",
                "We will see how EigenFaces can be used for Face Recognition using OpenCV in a later module this week.\n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">References and Further Reading<\/font>\n",
                "\n",
                "1. [https:\/\/docs.opencv.org\/3.3.1\/da\/d60\/tutorial_face_main.html](https:\/\/docs.opencv.org\/3.3.1\/da\/d60\/tutorial_face_main.html)\n",
                "\n",
                "2. [https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis)\n",
                "\n",
                "3. [https:\/\/en.wikipedia.org\/wiki\/Eigenface](https:\/\/en.wikipedia.org\/wiki\/Eigenface)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}