{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style = \"color:rgb(50,120,229)\">SnapChat Filters : Deformations<\/font>\n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">Moving Least Squares<\/font>\n",
                "\n",
                "\n",
                "In this lecture, we will discuss how to design Snapchat filters based on image deformation. We will use Moving Least Squares (MLS) technique to perform nonlinear deformations on an image.\n",
                "\n",
                "<table>\n",
                "    <tr>\n",
                "        <th><center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-horseImage.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-horseImage.png\"\/><\/a><\/center><\/th>\n",
                "        <th><center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-horseMLS.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-horseMLS.png\"\/><\/a><\/center><\/th>\n",
                "    <\/tr>\n",
                "    <tr>\n",
                "        <td colspan=\"2\">Moving Least Squares is a technique for warping an image based on a few control points. In the above image a few control points, shown using blue circles, are defined. Three of them are moved to produce a horse with an elongated neck. \n",
                "\n",
                "The above image was generated using the following online tool\n",
                "\n",
                "http:\/\/chenxing.name\/fun\/imgwarp-js\/\n",
                "        <\/td>\n",
                "    <\/tr>\n",
                "<\/table>\n",
                "\n",
                "An image can be deformed in an infinite number of ways. Theoretically, every pixel in an image can be independently moved to a different position. Like many things in life, we do not want total control, because absolute control over every single detail can be overwhelming. We just want a few knobs that give us enough control to achieve what we want. \n",
                "\n",
                "In affine transform, we have 6 knobs. They are **rotation** ( 1 parameter ), **translation** ( 2 parameters ), **scale** ( two parameters ), and **shear** ( 1 parameter ). \n",
                "\n",
                "The affine transform is a linear transform and it is useful in many applications. However, in many warping application we want nonlinear deformation. \n",
                "\n",
                "**Moving Least Squares** (MLS) is one such kind of deformation. It allows us to control warping using a few control points defined over the image. The control points are moved to produce the deformation. \n",
                "\n",
                "In the figure above, we have shown an example of MLS deformation. On the left is the original image of a horse with control points shown using blue circles. On the right image, three control points on the head are moved to produce an elongated neck. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Properties of MLS<\/font>\n",
                "\n",
                "Let\u2019s say there are $n$ control points $p_1, p_2, \\ldots p_n$ and they are moved to new locations given by $q_1, q_2, \\ldots q_n$. The warping function $f$ defined by the MLS has these three properties\n",
                "\n",
                "1. **Interpolation**: The control points $p_i$ should be directly mapped to $q_i$. In other words, $f(p_i) = q_i$\n",
                "\n",
                "2. **Smoothness**: $f$ should be a smooth function. \n",
                "\n",
                "3. **Identity** : If the point $p_i$ is not moved, the the deformation should be identity. In other words, $f(p_i) = p_i$\n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">How do you choose control points in MLS?<\/font>\n",
                "\n",
                "In MLS, the warp is calculated so that when it is applied to the input control points, we get the output control points. There are two kinds of regions where you should add control points. \n",
                "\n",
                "1. Region you want to deform. Let\u2019s say in a fun application called \"Pinocchio\", you want to elongate the nose. You should put a control point on the tip of the nose. \n",
                "\n",
                "2. Region you do not want to deform : In the same application, you may want to make sure the rest of the face is not deformed. So you put a few control points on the face. You may also want to make sure the boundary of the image is not distorted. So you put a few control points there as well. \n",
                "\n",
                "Let\u2019s look at an example. For centuries, scholars have debated Monalisa\u2019s mysterious smile. Some even doubt if she is smiling. We are going to recruit her in demonstrating MLS and put an end to this age old debate. \n",
                "\n",
                "\n",
                "\n",
                "| <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisa.jpg\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisa.jpg\" width=500\/><\/a><\/center> | <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisaSmile.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisaSmile.png\" width=500\/><\/a><\/center> | \n",
                "| -------- | -------- | \n",
                " \n",
                "In the Figure above, on the left is Monalisa with her mysterious smile. On the right is a smiling Monalisa we want to create by applying MLS to the original image. To accomplish this,  we choose a few control points shown in the left image. We have a few control points on the face and some on the boundary of the image. \n",
                "\n",
                "| <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisaControlPoints.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisaControlPoints.png\" width=500\/><\/a><\/center> | <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisaSmileConrolPoints.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-monalisaSmileConrolPoints.png\" width=500\/><\/a><\/center> | \n",
                "| -------- | -------- | \n",
                " \n",
                "\n",
                "Next, we move only two control points on the lips to make her smile. Because the other control points have not moved, the image does not deform in those locations. \n",
                "\n",
                "On the other hand, because we did not put any control points on the forehead, it is slightly broader than the original image. \n",
                "\n",
                "# <font style = \"color:rgb(50,120,229)\">MLS based SnapChat Filters<\/font>\n",
                "\n",
                "We will now discuss two SnapChat filters based on MLS. As you can imagine, there are many funny filters you can design with the knowledge of MLS, but the basic steps are the same. These are the common steps. \n",
                "\n",
                "1. Get landmark points using Dlib.\n",
                "\n",
                "2. Specify deformation: This is done by specifying two sets of control points. First, the points which will be fixed in the original and deformed image. Since they are fixed points, we call them **anchor points**. Second, the set of corresponding points ( shown in blue and red in Figure 1 below ) which will be deformed. We call this set of of points the **deformation points**. This is the only step that is different among different MLS based filters. \n",
                "\n",
                "3. Apply MLS based deformation. \n",
                "\n",
                "## <font style = \"color:rgb(50,120,229)\">Fatify Filter<\/font>\n",
                "\n",
                "<center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-hileriClintonCOntrolPoints.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-hileriClintonCOntrolPoints.png\" width=400\/><\/a><\/center>\n",
                "\n",
                "Figure 1 : Green points represent anchor points. The red points represent deformation points in the input image. The blue points represent the corresponding deformed points in the output image. \n",
                "\n",
                "\n",
                "\n",
                "The anchor points used in the fatify filter are shown using the green dots. \n",
                "\n",
                "The deformation points in the original image are shown using red dots. The corresponding deformation points in the deformed output image are shown using blue dots. \n",
                "\n",
                " \n",
                "\n",
                "For obtaining the blue points from the red points, we use the point on the tip of the nose. We draw a line from the tip of the nose to every red control point and then scale this vector by a value (offset). \n",
                "\n",
                "Once all the control points ( anchor points + deformation points ) are defined in both the input and the output image, MLS can be used to warp the entire image. \n",
                "\n",
                "### <font style = \"color:rgb(8,133,37)\">Fatify Code and Tutorial<\/font>\n",
                "\n",
                "The following tutorial shows Fatify filter for an image. The code for videos is similar and is included in the code section. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2,dlib,time,dlib\n",
                "import numpy as np\n",
                "import mls as mls\n",
                "import faceBlendCommon as fbc\n",
                "import matplotlib.pyplot as plt\n",
                "from dataPath import DATA_PATH\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib\n",
                "matplotlib.rcParams['figure.figsize'] = (6.0,6.0)\n",
                "matplotlib.rcParams['image.cmap'] = 'gray'\n",
                "matplotlib.rcParams['image.interpolation'] = 'bilinear'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "mls.GRID = 80"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Function to add the boundary points of an image to a given list of points "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to add boundary points of the image to the given \n",
                "# set of points\n",
                "def addBoundaryPoints(cols, rows, points):\n",
                "  # include the points on the boundaries\n",
                "  points = np.append(points,[[0, 0]],axis=0)\n",
                "  points = np.append(points,[[0, cols-1]],axis=0)\n",
                "  points = np.append(points,[[rows-1, 0]],axis=0)\n",
                "  points = np.append(points,[[rows-1, cols-1]],axis=0)\n",
                "  points = np.append(points,[[0, cols\/2]],axis=0)\n",
                "  points = np.append(points,[[rows\/2, 0]],axis=0)\n",
                "  points = np.append(points,[[rows-1, cols\/2]],axis=0)\n",
                "  points = np.append(points,[[rows\/2, cols-1]],axis=0)\n",
                "  return points"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Variables for resizing the image and dlib are initialized here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variables for resizing to a standard height\n",
                "RESIZE_HEIGHT = 360\n",
                "FACE_DOWNSAMPLE_RATIO = 1.5\n",
                "\n",
                "# Varibales for Dlib \n",
                "modelPath = DATA_PATH + \"models\/shape_predictor_68_face_landmarks.dat\"\n",
                "detector = dlib.get_frontal_face_detector()\n",
                "predictor = dlib.shape_predictor(modelPath)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This variable specifies the amount of bulge which will be applied to the chin points to get the fatify effect.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Amount of bulge to be given for fatify\n",
                "offset = 1.5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we specify the control points.\n",
                "\n",
                "The anchor points are those which will be fixed in both the original and deformed image. Deformed points are those which will undergo deformation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Points that should not move\n",
                "anchorPoints = [1, 15, 30]\n",
                "\n",
                "# Points that will be deformed\n",
                "deformedPoints = [ 5, 6, 8, 10, 11]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Read and resize the image and Get the landmarks using dlib."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "t = time.time()\n",
                "\n",
                "# Read an image and get the landmark points\n",
                "filename = DATA_PATH + '\/images\/hillary_clinton.jpg'\n",
                "\n",
                "src = cv2.imread(filename)\n",
                "height, width = src.shape[:2]\n",
                "\n",
                "IMAGE_RESIZE = np.float32(height)\/RESIZE_HEIGHT\n",
                "\n",
                "src = cv2.resize(src,None,\n",
                "                   fx=1.0\/IMAGE_RESIZE, \n",
                "                   fy=1.0\/IMAGE_RESIZE, \n",
                "                   interpolation = cv2.INTER_LINEAR)\n",
                "landmarks = fbc.getLandmarks(detector, predictor, \n",
                "                             src, FACE_DOWNSAMPLE_RATIO)\n",
                "\n",
                "print(\"Landmarks calculated in {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We specify the center to be the tip of the nose. The points will be pushed radially outwards using this as the pivot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the center of face to be the nose tip\n",
                "centerx, centery = landmarks[30][0], landmarks[30][1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here, we create a list of all control points on the original and deformed image. We also add the boundary points of the image to this list to prevent the image boundaries from getting warped."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variables for storing the original and deformed points\n",
                "srcPoints = []\n",
                "dstPoints=[]\n",
                "\n",
                "# Adding the original and deformed points using the landmark points\n",
                "for idx in anchorPoints:\n",
                "  srcPoints.append([landmarks[idx][0], landmarks[idx][1]])\n",
                "  dstPoints.append([landmarks[idx][0], landmarks[idx][1]])\n",
                "\n",
                "for idx in deformedPoints:\n",
                "  srcPoints.append([landmarks[idx][0], landmarks[idx][1]])\n",
                "  dstPoints.append([offset*(landmarks[idx][0] - centerx) + centerx, \n",
                "                    offset*(landmarks[idx][1] - centery) + centery])\n",
                "\n",
                "# Converting them to numpy arrays\n",
                "srcPoints = np.array(srcPoints)\n",
                "dstPoints = np.array(dstPoints)\n",
                "\n",
                "# Adding the boundary points to keep the image stable globally\n",
                "srcPoints = addBoundaryPoints(src.shape[0],src.shape[1],srcPoints)\n",
                "dstPoints = addBoundaryPoints(src.shape[0],src.shape[1],dstPoints)\n",
                "\n",
                "print(\"Points gathered {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-sourceAndDeformedPoints.jpg\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-sourceAndDeformedPoints.jpg\" width=400\/><\/a><\/center>\n",
                "\n",
                "&nbsp;\n",
                "Green points are the source Points and red points are the deformed points. The deformed points are just the stretched out versions of the green points on the chin region.\n",
                "\n",
                "We call the function to perform the deformation based on Moving Least Squares technique. \n",
                "\n",
                "The arguments are\n",
                "- **`src`** - source image\n",
                "- **`srcPoints`** - control points on the source image \n",
                "- **`dst`** - destination image\n",
                "- **`dstPoints`** - deformed points on the destination image "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performing moving least squares deformation on the image using the \n",
                "# points gathered above\n",
                "dst = mls.MLSWarpImage(src, srcPoints, dstPoints)\n",
                "\n",
                "print(\"Warping done {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display and save the images\n",
                "combined = np.hstack([src, dst])\n",
                "\n",
                "plt.figure(figsize = (15,15))\n",
                "plt.imshow(combined[:,:,::-1])\n",
                "plt.title('Fatify Filter')\n",
                "plt.axis('off')\n",
                "\n",
                "print(\"Total time {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style = \"color:rgb(50,120,229)\">Happify Filter<\/font>\n",
                "\n",
                "In the Happify filter, the deformation points in the original image are shown in red and the corresponding points in the deformed image are shown in blue. See Figure 2 below. The blue control points in the upper part of the face are calculated by extending the line joining a point on the bridge of the nose to the red control points by an offset. Similarly, the blue control points  on the lower half of the face are calculated by extending the line joining the chin to the red control points by an offset value. \n",
                "\n",
                "The anchor points are shown in green in Figure 2. \n",
                "\n",
                "| <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-deformatedRedPoints.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-deformatedRedPoints.png\" width=500\/><\/a><\/center> | <center><a href=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-anchorPoints.png\"><img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/01\/opcv4face-w5-m5-anchorPoints.png\" width=500\/><\/a><\/center> | \n",
                "| -------- | -------- |\n",
                "\n",
                "<center>Figure 2 : Left: The deformation of the red points ( shown using blue points ) is calculated by extending the link joining the yellow points to the red points by an offset. <\/center>\n",
                "\n",
                "Right: Green points represent anchor points. The red points represent deformation points in the input image. The blue points represent the corresponding deformed points in the output image. \n",
                "\n",
                "\n",
                "\n",
                "### <font style = \"color:rgb(8,133,37)\">Happify Code and Tutorial<\/font>\n",
                "\n",
                "The following tutorial shows Happify filter for an image. The code for videos is similar and is included in the code section. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Function for calculating eight boundary points."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to add boundary points of the image to the given set of\n",
                "#  points\n",
                "def addBoundaryPoints(cols, rows, points):\n",
                "  # include the points on the boundaries\n",
                "  points = np.append(points,[[0, 0]],axis=0)\n",
                "  points = np.append(points,[[0, cols-1]],axis=0)\n",
                "  points = np.append(points,[[rows-1, 0]],axis=0)\n",
                "  points = np.append(points,[[rows-1, cols-1]],axis=0)\n",
                "  points = np.append(points,[[0, cols\/2]],axis=0)\n",
                "  points = np.append(points,[[rows\/2, 0]],axis=0)\n",
                "  points = np.append(points,[[rows-1, cols\/2]],axis=0)\n",
                "  points = np.append(points,[[rows\/2, cols-1]],axis=0)\n",
                "  return points"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Image resize parameters and MLS grid size. \n",
                "\n",
                "MLS is expensive to calculate at every pixel. So it is calculated over a grid much bigger than a pixel. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variables for resizing to a standard height\n",
                "RESIZE_HEIGHT = 360\n",
                "FACE_DOWNSAMPLE_RATIO = 1.5\n",
                "\n",
                "# MLS grid size\n",
                "mls.GRID = 30"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load face detector and landmark detector"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Varibales for Dlib \n",
                "modelPath = DATA_PATH + \"models\/shape_predictor_68_face_landmarks.dat\"\n",
                "faceDetector = dlib.get_frontal_face_detector()\n",
                "landmarkDetector = dlib.shape_predictor(modelPath)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The variables `offset1` and `offset2` specify the multiplier used for deforming the upper and the lower face. Larger values of offset would cause the blue dots to be located further away from the red dots leading to higher deformation. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parameters for controlling the amount of deformation \n",
                "offset1 = 1.5\n",
                "offset2 = 1.5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The tip of the nose and the chin are chosen as anchor points. Later the boundary points will also be added as anchor points. \n",
                "\n",
                "A few points on the lower face and a few on the upper face are chosen for deformation. In Figure 2, these points are represented by the red dots.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Points that should not move\n",
                "anchorPoints = [8, 30]\n",
                "\n",
                "# Points that will be deformed\n",
                "# For lips\n",
                "deformedPoints1 = [48, 57, 54]\n",
                "# For eyes\n",
                "deformedPoints2 = [21, 22, 36, 45]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Read and resize the image and get the landmarks using dlib."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": [
                "t = time.time()\n",
                "# Read an image and get the landmark points\n",
                "filename = DATA_PATH + '\/images\/ted_cruz.jpg'\n",
                "src = cv2.imread(filename)\n",
                "height, width = src.shape[:2]\n",
                "IMAGE_RESIZE = np.float32(height)\/RESIZE_HEIGHT\n",
                "src = cv2.resize(src,None,\n",
                "                   fx=1.0\/IMAGE_RESIZE, \n",
                "                   fy=1.0\/IMAGE_RESIZE, \n",
                "                   interpolation = cv2.INTER_LINEAR)\n",
                "landmarks = fbc.getLandmarks(faceDetector, landmarkDetector, src, \n",
                "                             FACE_DOWNSAMPLE_RATIO)\n",
                "\n",
                "print(\"Landmarks calculated in {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Two points `center1` and `center2` are chosen for calculating deformation of the lower face points and the upper face points respectively. These points are represented by the yellow points in Figure 2. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the center to tip of chin\n",
                "center1x, center1y = landmarks[8][0], landmarks[8][1]\n",
                "# Set the center to point on nose\n",
                "center2x, center2y = landmarks[28][0], landmarks[28][1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Collect all points ( anchor points + deformation points ) in a single array for both the source image ( `srcPoints` ) and destination image ( `dstPoints` ). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variables for storing the original and deformed points\n",
                "srcPoints = []\n",
                "dstPoints = []\n",
                "\n",
                "# Adding the original and deformed points using the landmark points\n",
                "for idx in anchorPoints:\n",
                "  srcPoints.append([landmarks[idx][0], landmarks[idx][1]])\n",
                "  dstPoints.append([landmarks[idx][0], landmarks[idx][1]])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The line joining `center1` and `center2` to the deformation points (red) is scaled by `offset1` and `offset2`  to produce the blue points. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "for idx in deformedPoints1:\n",
                "  srcPoints.append([landmarks[idx][0], landmarks[idx][1]])\n",
                "  dstPoints.append([offset1*(landmarks[idx][0] - center1x) + \n",
                "                    center1x, offset1*(landmarks[idx][1] - center1y)\n",
                "                    + center1y])\n",
                "\n",
                "for idx in deformedPoints2:\n",
                "  srcPoints.append([landmarks[idx][0], landmarks[idx][1]])\n",
                "  dstPoints.append([offset2*(landmarks[idx][0] - center2x) + \n",
                "                    center2x, offset2*(landmarks[idx][1] - center2y)\n",
                "                    + center2y])\n",
                "\n",
                "# Converting them to numpy arrays\n",
                "srcPoints = np.array(srcPoints)\n",
                "dstPoints = np.array(dstPoints)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The boundary points are added to both `srcPoints` and `dstPoints`. They serve as additional anchor points."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding the boundary points to keep the image stable globally\n",
                "srcPoints = addBoundaryPoints(src.shape[0],src.shape[1],srcPoints)\n",
                "dstPoints = addBoundaryPoints(src.shape[0],src.shape[1],dstPoints)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Points gathered {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performing moving least squares deformation on the image using the \n",
                "# points gathered above\n",
                "dst = mls.MLSWarpImage(src, srcPoints, dstPoints)\n",
                "\n",
                "print(\"Warping done {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display and save the images\n",
                "combined = np.hstack([src, dst])\n",
                "\n",
                "plt.figure(figsize = (15,15))\n",
                "plt.imshow(combined[:,:,::-1])\n",
                "plt.title(\"Happify Filter\")\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "\n",
                "print(\"Total time {}\".format(time.time() - t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style = \"color:rgb(50,120,229)\">References and Further Reading<\/font>\n",
                "\n",
                "1. [http:\/\/chenxing.name\/fun\/imgwarp-js\/](http:\/\/chenxing.name\/fun\/imgwarp-js\/)\n",
                "\n",
                "2. [http:\/\/faculty.cs.tamu.edu\/schaefer\/research\/mls.pdf](http:\/\/faculty.cs.tamu.edu\/schaefer\/research\/mls.pdf)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}